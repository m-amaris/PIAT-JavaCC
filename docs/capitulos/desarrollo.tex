\section{Introducción}
\noindent En el capítulo anterior, exploramos las bases tecnológicas y funcionalidades que sustentan el proyecto de evaluación de la viabilidad de JavaCC para el procesamiento de información en la asignatura PIAT. En este nuevo capítulo, nos embarcaremos en un viaje más profundo, adentrándonos en los detalles de la implementación del proyecto, los procedimientos meticulosamente seguidos y el entorno cuidadosamente seleccionado para los desarrollos y las pruebas.

A lo largo de las siguientes secciones, desentrañaremos el proceso de implementación, desglosando los pasos específicos emprendidos durante el desarrollo y revelando el entorno de trabajo cuidadosamente diseñado para garantizar una integración y ejecución exitosas del proyecto. Este recorrido nos permitirá comprender cómo se ha dado vida a esta iniciativa, proporcionando una visión completa de los esfuerzos realizados para alcanzar sus objetivos.

Preparémonos para sumergirnos en el corazón del proyecto, explorando las decisiones tomadas, las herramientas utilizadas y los desafíos superados en el camino hacia la implementación exitosa de JavaCC en PIAT.

\section{Implementación}
\noindent La implementación del proyecto se ha llevado a cabo utilizando el lenguaje de programación Java, aprovechando las capacidades y herramientas que nos proporciona para lograr un desarrollo eficiente y robusto. En esta sección y en las posteriores, se profundizará en los detalles de la implementación, destacando las tecnologías clave utilizadas y las decisiones de diseño tomadas para alcanzar los objetivos del proyecto. 

Las tecnologías clave son JavaCC, que nos va a permitir crear analizadores para lenguajes formales; Java, que nos ofrece características como la portabilidad, orientación a objetos y gestión automática de memoria; y un Entorno de desarrollo Integrado (IDE), que nos va a proporcionar las herramientas para desarrollar las prácticas, como el editor de código, compilador, depurador. Hablaremos del entorno utilizado más adelante.

\section{Procedimientos}

\noindent El proceso de implementación se ha dividido en las siguientes fases:

\phantom{text}

\noindent \textbf{Fase 1: Aprendizaje de los conceptos básicos de JavaCC.}

\phantom{text}

\noindent La primera fase consiste en adquirir los conocimientos básicos sobre el funcionamiento de JavaCC, sus conceptos ---qué es un token, un estado léxico, un no-terminal, entre otros--- y su sintaxis, incluyendo la definición de gramáticas, la creación de los analizadores léxicos y sintácticos y la generación del código Java. Esta fase se corresponde con el \hyperref[sec:cap2]{\textit{Capítulo 2. Estado del arte}} de este \pfg. Esta fase es considerablemente amplia, ya que hay que dedicarle el tiempo necesario a aprender e interiorizar todos los conceptos y el funcionamiento con esta herramienta.

\phantom{text}

\noindent \textbf{Fase 2: Práctica con JavaCC para crear analizadores léxicos y sintácticos para gramáticas simples.}

\phantom{text}
    
\noindent En la siguiente fase, se realizan ejercicios prácticos para familiarizarse con la herramienta JavaCC y desarrollar la habilidad de crear analizadores léxicos y sintácticos para gramáticas simples. Uno de los ejemplos fue el presentado en la sección \hyperref[sec:ampliacioncalculadora]{\textit{2.2.5. Ejemplo de gramática en JavaCC - Ampliación de la funcionalidad a Calculadora}}. Realizar estos ejercicios es esencial antes de intentar realizar prácticas más elaboradas, ya que permiten poner en práctica las funcionalidades aprendidas de una manera más simple y práctica.

\phantom{text}

\noindent \textbf{Fase 3: Aplicación de JavaCC en prácticas de PIAT.}

\phantom{text}

\noindent Una vez se tiene un grado notable de experiencia utilizando la herramienta, en esta fase se aplica JavaCC para desarrollar los analizadores léxicos y sintácticos necesarios para resolver las prácticas de la asignatura PIAT. Esta fase es la más importante y la que más tiempo requiere, ya que se realiza una reingeniería de la práctica, adaptando el código original que utilizaba la herramienta actual a una nueva implementación basada en JavaCC. Este proceso implica analizar la lógica a realizar, identificar las reglas gramaticales que definen la estructura del documento o fichero y traducirlas a una gramática JavaCC.

\phantom{text}

\noindent \textbf{Fase 4: Generación de documentación.}

\phantom{text}

\noindent Por último, en la fase 4 se refleja todo el trabajo realizado en las fases anteriores, desde la generación de las gramáticas y su aplicación hasta la validación de resultados y las conclusiones extraídas en cada ejercicio.


\section{Entorno}
\noindent Los desarrollos y las pruebas se han realizado en un entorno de desarrollo integrado (IDE) de Java. En concreto, se ha empleado Eclipse IDE ya que, además de ser el IDE principal con el que se realizan las prácticas de PIAT ---y en general, cualquier práctica relacionada con la programación dentro de la escuela---, existe un plugin de JavaCC que realiza la compilación de los archivos y facilita enormemente el desarrollo. Dicho esto, el lector puede optar por elegir otros editores como Visual Studio Code o Jetbrains, incluso editores de texto como Notepad++, Vim o  nano. Este aspecto queda a gusto del desarrollador. Para la depuración de los analizadores se ha utilizado el debugger del IDE.

Además, para poder realizar los desarrollos necesarios se necesita descargar e instalar la herramienta JavaCC en el equipo. Para saber mas acerca de la instalación y configuración de Elipse IDE con JavaCC, puede consultar el anexo \hyperref[sec:instalaciondejavacc]{\textit{Instalación de JavaCC}}.

En cuanto al equipo de pruebas, se ha elegido un procesador Intel de 8ª generación, el cual cuenta con 6 núcleos y 12 hilos ---este dato es importante a la hora de realizar la validación de resultados en las diferentes prácticas-- con una memoria RAM de 16GB. Estos datos se proporcionan de tal forma que el lector tenga conocimiento del rendimiento de los distintos analizadores en función de  las especificaciones del equipo utilizadas.

\section{Objetivos de la implementación}
\noindent Entre los objetivos principales de la implementación se encuentran el implementar los analizadores léxico y sintáctico para las gramáticas de las prácticas de PIAT, el evaluar la viabilidad de utilizar JavaCC para abordar las prácticas de PIAT, y generar documentación que sirva como recurso para estudiantes y profesores interesados en utilizar JavaCC en proyectos relacionados con el procesamiento de información.

% [COMPLETAR] Una vez claros el entorno utilizado, las especificaciones y configuraciones implementadas en el proyecto, y vistos los objetivos a conseguir en las prácticas, se procederá a analizar el primer caso práctico en el que se puede aplicar JavaCC.

Con el fin de alcanzar estos objetivos, se implementarán analizadores léxicos y sintácticos utilizando JavaCC para cada una de las cinco prácticas de PIAT seleccionadas. Se analizará el rendimiento de JavaCC en comparación con las herramientas tradicionales utilizadas en cada práctica, documentando las ventajas, desventajas y particularidades de su aplicación. Finalmente, se elaborará una guía completa que detalle el proceso de implementación de JavaCC en cada práctica, incluyendo ejemplos de código y explicaciones claras para facilitar su comprensión y uso por parte de estudiantes y profesores.

\section{Análisis de ficheros de \textit{log}. Práctica 2}

\noindent El mundo digital genera una gran cantidad de datos, y los logs de sistemas son una fuente importante de información para comprender el comportamiento y el rendimiento de estos sistemas. La práctica 2 de PIAT nos invita a explorar este mundo de datos mediante el análisis de logs de un sistema de correo electrónico. El objetivo de esta práctica es el de aplicar los conocimientos adquiridos sobre expresiones regulares y hacer uso de la programación Java para analizar y extraer información de logs de dicho sistema.

Concretamente, el objetivo de la practica es utilizar expresiones regulares para extraer información estadística y generar informes a partir de los archivos de log del sistema de correo electrónico. La arquitectura del sistema y el formato de los logs se detallan en los anexos \hyperref[sec:P2SistemaCorreo]{\textit{Descripción general del sistema de correo electrónico}} y \hyperref[sec:logscorreo]{\textit{Formato de logs. Sistema de correo electrónico}}.


\subsection{Expresiones Regulares. RegEx}

\noindent Las expresiones regulares, también conocidas como Regex, son un lenguaje formal para describir patrones en cadenas de texto. Son como lupas que nos permiten enfocarnos en detalles específicos dentro de un gran volumen de texto, permitiéndonos extraer información relevante de manera eficiente y precisa.

En esta práctica, las expresiones regulares serán nuestras aliadas para navegar por el laberinto de los logs, desentrañando los mensajes y eventos registrados por el sistema de correo electrónico. Definiremos expresiones regulares para identificar diferentes tipos de registros, como mensajes entrantes, mensajes salientes, mensajes infectados, códigos de error y mucho más.

En este contexto, aplicaremos las expresiones regulares para definir los distintos tokens que queremos reconocer y asi conformar la gramática que analizará JavaCC.

\subsection{Descripción de la práctica}

\noindent La práctica se divide en tres partes principales: La obtención de estadísticos generales, cuya función es contabilizar el número de servidores procesados, los  archivos procesados registros procesados y registros con errores de formato; El análisis por tipo de servidor y día, en el que para cada tipo de servidor, se debe calcular y mostrar los siguientes estadísticos por día:

\begin{itemize}
    \item \lstinline|msgIn|: Número de mensajes entrantes.
    \item \lstinline|msgOut|: Número de mensajes salientes.
    \item \lstinline|msgINFECTED|: Número de mensajes infectados con virus.
    \item \lstinline|msgSPAM|: Número de mensajes SPAM no bloqueados.
    \item \lstinline|code 4.3.2|: Número de intentos de entrega con código de estado 4.3.2 (sobrecarga).
    \item \lstinline|code 5.1.1|: Número de intentos de entrega de mensajes  entrantes con código de estado 5.1.1 (dirección de correo electrónico de destino incorrecta).
\end{itemize}

; Y por ultimo, identificar aquellas cuentas de correo internas desde las que se han enviado más de 500 mensajes. Para cada cuenta, se debe mostrar el nombre de usuario y el número de mensajes enviados.

Un ejemplo del resultado a obtener sería el siguiente:

\lstset{inputencoding=utf8/latin1}
\lstinputlisting{code/P2salida.txt}

\subsection{Desarrollo de la práctica}

Una vez comprendido para que se utilizan las expresiones regulares y conocido el contexto de la práctica con el objetivo final, en esta sección se describe cómo se resuelve esta práctica utilizando JavaCC, apoyándonos en el archivo \hyperref[sec:P2Parser]{Parser.jjz} proporcionado.

El primer paso para resolver la práctica es definir la estructura de árbol que seguirá el analizador sintáctico. Esta estructura determina el tipo de traza que se está analizando y las acciones correspondientes a realizar en cada caso. La \autoref{fig:arboltraza} ilustra esta estructura:

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{imagenes/arboltraza.png}
    \caption{Estructura de árbol. Análisis de trazas log.}
    \label{fig:arboltraza}
\end{figure}

Esta figura refleja la estructura del análisis sintáctico, en la que se observa toda la estructura definida a la hora de reconocer las trazas. Se observa como se contemplan cinco tipos de trazas, las \lstinline|msa|, \lstinline|smpt-in|, \lstinline|smpt-out|, \lstinline|security| y otros tipos de servidores. Para cada tipo de traza se definen las distintas posibilidades que puede haber dentro de cada tipo de traza, de tal forma que se describe la estructura de forma detallada. En aquellas ``ramas'' en las que queramos ejecutar alguna acción, como puede ser añadir los valores a un estadístico, se define una acción léxica dentro del no-terminal, como por ejemplo:

\lstset{inputencoding=utf8/latin1}
\lstinputlisting{code/overload.jj}

El archivo Parser.jj define el analizador léxico y sintáctico para procesar los logs. A continuación, se detalla la estructura y las funciones más relevantes del archivo.

En las opciones del analizador se definen configuraciones importantes como la concurrencia, ya que la gramática generada en JavaCC debe tener la capacidad de ejecutarse de forma concurrente para distintas entradas ---en este caso, varios ficheros de log---

\lstset{inputencoding=utf8/latin1}
\lstinputlisting{code/p2static.jj}

además de incluir varias declaraciones y métodos auxiliares que facilitan el registro y manejo de estadísticas:

\lstset{inputencoding=utf8/latin1}
\lstinputlisting{code/p2declarations.jj}

Los tokens y estados léxicos se definen para identificar y manejar las distintas partes del log. Aquí se muestra un extracto de estas definiciones:

\lstset{inputencoding=utf8/latin1}
\lstinputlisting{code/p2tokens.jj}

Cabe destacar que se utilizan estados léxicos como \lstinline|SERVER|, \lstinline|LEX_MSA|, \lstinline|LEX_IN|, \lstinline|LEX_OUT|... que permiten analizar unicamente ciertos tokens cuando el analizador se encuentra en dicho estado, lo que acelera de forma desmesurada la rapidez del analizador ---lo veremos en la sección de validación de resultados---.

% \textbf{...diagrama o tabla que resuma los estados léxicos y su función...
% }
%  Esto facilitaría la comprensión de la estructura del analizador

% El analizador léxico utiliza diferentes estados para reconocer los tokens de entrada de manera eficiente y precisa.

La \autoref{fig:tablaestadoslexicosp2} resume los diversos estados léxicos utilizados. El estado inicial es \lstinline|DEFAULT|, donde se reconocen tokens comunes como espacios en blanco, saltos de línea y fechas.

Al encontrar un token que indica el inicio de un tipo específico de servidor, el analizador cambia a un estado léxico específico para ese servidor. Por ejemplo, al encontrar el token \lstinline|msa|, el analizador cambia al estado \lstinline|LEX_MSA|. Dentro de cada estado de servidor (\lstinline|LEX_MSA|, \lstinline|LEX_IN|, \lstinline|LEX_OUT|, \lstinline|LEX_SEC|), se reconocen tokens específicos relacionados con ese tipo de servidor, como \lstinline|message from:| en \lstinline|LEX_MSA| o \lstinline|bounced from:| en \lstinline|LEX_IN|.

El estado \lstinline|VALIDA| se utiliza para reconocer componentes de identificadores de servidor válidos, como letras, números y ciertos caracteres especiales. Este estado se utiliza junto con los estados de servidor para asegurar que los identificadores de servidor se analizan correctamente.

Además de los estados de servidor, el analizador también utiliza estados léxicos para manejar casos especiales dentro de ciertos servidores. Por ejemplo, \lstinline|LEX_IN| tiene tokens específicos para manejar mensajes rebotados, mientras que \lstinline|LEX_SEC| tiene tokens para identificar mensajes infectados o spam.

Finalmente, el estado \lstinline|UNEXPECTED| captura cualquier carácter que no coincida con ningún otro token definido, lo que permite al analizador manejar errores de sintaxis de manera más robusta.

\begin{figure}[H]
  \centering
  \begin{tabularx}{\textwidth}{>{\bfseries}l X X}
  \toprule
  \textbf{Estado Léxico} & \textbf{Descripción} & \textbf{Función} \\
  \midrule
  DEFAULT & Estado inicial & Reconoce tokens comunes como espacios en blanco, saltos de línea, fechas y cualquier otra secuencia que no coincida con una fecha \\
  \midrule
  SERVER & Activado después de una hora & Reconoce los diferentes tipos de servidores y sus identificadores \\
  \midrule
  LEX\_MSA & Activado después de \lstinline|msa| & Reconoce tokens específicos de servidores \lstinline|MSA|\\
  \midrule
  LEX\_IN & Activado después de reconocer \lstinline|smtp-in|& Reconoce tokens específicos de servidores \lstinline|SMTP-IN|\\
  \midrule
  LEX\_OUT & Activado después de reconocer \lstinline|smtp-out|& Reconoce tokens específicos de servidores \lstinline|SMTP-OUT| \\
  \midrule
  LEX\_SEC & Activado después de reconocer \lstinline|security-in| o \lstinline|security-out| & Reconoce tokens específicos de servidores \lstinline|SEC| \\
  \midrule
  VÁLIDA & Activado después de un identificador de servidor & Reconoce caracteres especiales que pueden formar parte de un identificador \\
  \bottomrule
  \end{tabularx}
  % \caption{Descripción de los Estados Léxicos}
  \caption{\label{fig:tablaestadoslexicosp2}Práctica 2. Estados léxicos del analizador}
  \label{table:lexical_statesp2}
  \end{figure}

Por último, las producciones definen las reglas sintácticas que el analizador seguirá. A continuación se muestra una producción principal y una producción para manejar trazas del tipo \lstinline|MSA|. Estas producciones se corresponden con la estructura mencionada anteriormente (\autoref{fig:arboltraza}):

\lstset{inputencoding=utf8/latin1}
\lstinputlisting{code/p2productions.jj}

\subsection{Validación de resultados}

\noindent Para validar los resultados obtenidos, se han comparado los informes generados con los esperados. Además, se han realizado pruebas con diferentes conjuntos de datos de log para verificar la robustez del analizador, asi como varias pruebas de tiempo para comparar el tiempo de ejecución usando regEx y utilizando JavaCC. 

De las pruebas realizadas, la más reseñable es la comparativa entre las dos herramientas en cuanto a rendimiento se refiere para el análisis de los logs: expresiones regulares tradicionales y JavaCC. Los resultados obtenidos reflejan una mejora significativa en el tiempo de ejecución cuando se utilizó JavaCC.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{imagenes/comparacion_tiempos_ejecucion.png}
\caption{Comparación de Tiempos de Ejecución: Expresiones Regulares vs JavaCC}
\label{fig:comparacion_tiempos_ejecucion.png}
\end{figure}

\noindent En la gráfica de la \autoref{fig:comparacion_tiempos_ejecucion.png} se puede observar que se han realizado 100 ejecuciones para determinar el tiempo de ejecución en ambas herramientas. Empleando expresiones regulares tradicionales los valores son de aproximadamente 11 segundos, mientras que utilizando JavaCC el tiempo de ejecución medio se redujo a 2 segundos.

La reducción significativa en el tiempo de ejecución al utilizar JavaCC puede cuantificarse de la siguiente manera: JavaCC fue \textbf{aproximadamente un 82\% más rápido} que las expresiones regulares tradicionales.

Esta mejora se debe a varios factores:

\begin{itemize}
\item Eficiencia en la Compilación de Gramáticas: JavaCC permite definir gramáticas de manera más estructurada y eficiente, optimizando el proceso de análisis léxico y sintáctico.
\item Menor Sobrecarga de Procesamiento: Las expresiones regulares, aunque útiles, son costosas en términos de tiempo de procesamiento, especialmente cuando se aplican repetidamente a grandes volúmenes de datos. JavaCC, al generar analizadores más optimizados, reduce esta sobrecarga. Además, en este práctica, todas las expresiones regulares definidas son ejecutadas por cada traza ---si se definen siete expresiones regulares a analizar, por cada traza que se quiera comprobar, habrá que ejecutar las siete expresiones para comprobar si coincide o no--- , mientras que empleando JavaCC, cada traza es analizada una única vez, siguiendo la estructura de árbol presentada en la Figura \ref{fig:comparacion_tiempos_ejecucion.png}. Este es el motivo principal por el que se observa una gran diferencia entre los dos tiempos de ejecución.
\item Optimización del Código Generado: El código generado por JavaCC está específicamente optimizado para el reconocimiento y procesamiento de patrones definidos en la gramática, lo que resulta en una ejecución más rápida.
\end{itemize}

Esta mejora en el rendimiento es especialmente relevante en aplicaciones que requieren el procesamiento de grandes volúmenes de datos en tiempo real o casi real, donde cada segundo cuenta. En el contexto de la práctica, la utilización de JavaCC no solo mejoró la eficiencia del análisis, sino que también facilitó la gestión y mantenimiento del código, proporcionando una solución más robusta y escalable.

\subsection{Conclusiones}

\noindent La práctica 2 de PIAT ha proporcionado una valiosa oportunidad para aplicar y profundizar en el uso de expresiones regulares y JavaCC en el análisis de archivos de log de un sistema de correo electrónico. A lo largo del desarrollo de esta práctica, se ha demostrado la importancia de las expresiones regulares para la extracción de información específica de los logs, permitiendo la generación de informes estadísticos detallados sobre el tráfico y los eventos registrados en el sistema de correo.

El uso de JavaCC ha sido especialmente destacable en esta práctica. Al implementar un analizador sintáctico con JavaCC, se ha logrado no solo una mayor precisión en la identificación y clasificación de los diferentes tipos de trazas, sino también una optimización significativa en el tiempo de procesamiento. Los resultados obtenidos muestran que el uso de JavaCC ha reducido el tiempo de ejecución de 11 segundos, que era el tiempo necesario al utilizar expresiones regulares, a solo 2 segundos. Esta mejora del 82\% en el rendimiento destaca la eficiencia y la escalabilidad de JavaCC frente a las expresiones regulares tradicionales.

El proceso de análisis se estructuró en varias etapas, desde la obtención de estadísticas generales hasta el análisis detallado por tipo de servidor y día, culminando en la identificación de cuentas de correo con un alto volumen de mensajes enviados. La estructura de árbol definida para el analizador sintáctico ha sido fundamental para categorizar y procesar adecuadamente las trazas, permitiendo ejecutar acciones específicas según el tipo de traza identificado.

En términos de validación de resultados, la comparación entre los métodos de expresiones regulares y JavaCC ha sido crucial. La gráfica generada para múltiples ejecuciones muestra de manera clara y visual que JavaCC ofrece un rendimiento mucho más rápido y consistente. Este hallazgo subraya la importancia de elegir herramientas adecuadas para el análisis de grandes volúmenes de datos, especialmente en contextos donde el rendimiento y la eficiencia son críticos.

En conclusión, la implementación de esta práctica ha demostrado que JavaCC no solo mejora la precisión en el análisis de logs, sino que también ofrece una solución significativamente más eficiente en términos de tiempo de ejecución. Esta optimización es especialmente relevante en entornos de producción, donde la capacidad de procesar rápidamente grandes cantidades de datos puede marcar una diferencia crucial en la operatividad y la toma de decisiones. El éxito de esta práctica refuerza la importancia de adoptar tecnologías avanzadas y eficientes para el análisis de datos, contribuyendo a un mejor rendimiento y eficacia en la gestión de sistemas de información.

\subsection{Código Fuente}

\noindent El código fuente de los archivos elaborados en esta practica se encuentra a continuación para su estudio y aprendizaje. Cabe destacar que se proporcionan dicho código para que el lector sea capaz de probar las funcionalidades con el objetivo de aprender más acerca de las aplicaciones de JavaCC.

No se promueve el plagio y la no-realización de las prácticas por parte del alumnado, esta es simplemente una de las muchas formas en las que se podría realizar la práctica utilizando JavaCC.

\hyperref[sec:P2Parser]{\textbf{Parser.jj}}

\section{Análisis de archivos XML. Práctica 3}

\noindent La práctica 3 de PIAT, centrada en el procesamiento de documentos XML, representa un hito significativo en el marco de este proyecto. En esta etapa, se aborda la implementación de un analizador de documentos XML utilizando la tecnología SAX. El objetivo principal aprender la estructura de XML y su funcionamiento, familiarizando a los estudiantes con SAX y fortaleciendo sus habilidades en el diseño de algoritmos eficientes para la extracción y transformación de información a partir de fuentes de contenidos estructurados.

La práctica 3 de PIAT se centra en familiarizarse con la tecnología SAX y desarrollar un analizador de documentos XML basado en esta tecnología. Además, el objetivo secundario es diseñar algoritmos eficientes que permitan la extracción y transformación de información a partir de fuentes de contenidos estructurados.

\subsection{SAX}
\noindent La tecnología SAX (\textit{Simple API for XML}) es una API que se encarga de procesar documentos XML. Está basada en eventos, lo que quiere decir que el analizador XML notifica al programa cuando encuentra un evento especifico en el documento. El programa puede tomar medidas en función del evento. Los eventos que puede notificar SAX son:
\begin{itemize}
    \item Inicio de documento: Se produce cuando el analizador comienza a procesar el documento.
    \item Fin de documento: Se produce cuando el analizador finaliza el procesamiento del documento.
    \item Inicio de elemento: Se produce cuando el analizador encuentra el inicio de un elemento XML.
    \item Fin de elemento: Se produce cuando el analizador encuentra el final de un elemento XML.
    \item Caracteres: Se produce cuando el analizador encuentra caracteres no pertenecientes a un elemento XML.
    \item Error: Se produce cuando el analizador encuentra un error en el documento XML.
\end{itemize}

SAX es una buena opción para procesar documentos XML grandes o complejos. Esto se debe a que el analizador XML no necesita cargar todo el documento en memoria a la vez. En cambio, el analizador puede procesar el documento de forma secuencial, lo que puede ahorrar memoria.

La tecnología SAX también es una buena opción para procesar documentos XML que se actualizan con frecuencia. Esto se debe a que el analizador XML puede procesar solo los cambios en el documento, lo que puede ahorrar tiempo.

Algunos ejemplos de cómo se puede utilizar SAX para procesar documentos XML incluyen:
\begin{itemize}
    \item Leer un documento XML y extraer los datos que contiene.
    \item Validar un documento XML para asegurarse de que cumple con las reglas de un esquema XML.
    \item Transformar un documento XML a un formato diferente.
\end{itemize}

\subsection{JavaCC vs. SAX}

\noindent JavaCC y SAX son dos herramientas relativamente diferentes a la hora de procesar documentos XML. Como hemos estudiado en el capítulo anterior, JavaCC es una herramienta de generación de analizadores que permite crear analizadores personalizados, mientras que SAX es una API estándar que proporciona un conjunto de métodos para procesar documentos XML.
Son varias las razones por la que es más conveniente usar JavaCC en vez de SAX:
\begin{itemize}
    \item Control: JavaCC ofrece un mayor control sobre el proceso de análisis que SAX. Esto permite al desarrollador crear analizadores que se adapten a sus necesidades específicas.
    \item Eficiencia: JavaCC puede generar analizadores que sean más eficientes que los analizadores SAX estándar. Esto se debe a que JavaCC puede aprovechar las características del lenguaje Java para optimizar el proceso de análisis.
    \item Flexibilidad: JavaCC permite crear analizadores XML que sean más flexibles que los analizadores SAX estándar. Esto se debe a que JavaCC permite a los desarrolladores definir sus propias reglas de análisis.
\end{itemize}
En concreto, JavaCC ofrece las siguientes ventajas sobre SAX:
\begin{itemize}
    \item Puede generar analizadores personalizados que se adapten a las necesidades específicas del documento XML. Esto permite a los desarrolladores crear analizadores que sean más eficientes, precisos y fáciles de mantener.
    \item Puede generar analizadores que sean más eficientes que los analizadores SAX estándar. Esto se debe a que JavaCC puede aprovechar las características del lenguaje Java para optimizar el proceso de análisis.
    \item Puede generar analizadores que sean más flexibles que los analizadores SAX estándar. Esto se debe a que JavaCC permite a los desarrolladores definir sus propias reglas de análisis.
\end{itemize}

Sin embargo, JavaCC también tiene algunas desventajas con respecto a SAX:
\begin{itemize}
    \item Es más complejo de aprender y usar que SAX. Esto se debe a que JavaCC requiere conocimientos de programación en Java.
    \item Puede ser más lento que SAX para documentos XML pequeños. Esto se debe a que JavaCC debe generar un analizador personalizado para cada documento XML.
\end{itemize}

En general, JavaCC es una buena opción para procesar documentos XML cuando se necesita un mayor control, eficiencia o flexibilidad. Sin embargo, SAX es una buena opción para procesar documentos XML pequeños o cuando se necesita una solución rápida y fácil de usar.

\subsection{Descripción de la práctica}

% \noindent Una vez conocido la herramienta SAX para analizar archivos XML y visto el contexto de esta práctica, a continuación se presenta el enunciado de la práctica propuesta del año 2022:
% Se desea desarrollar una aplicación que extraiga cierta información de un fichero XML obtenido a partir de los datos publicados en el Portal de Datos Abiertos del Ayuntamiento de Madrid (http://datos.madrid.es).

% La información (organismos, eventos, actividades, …) publicada a través del Portal de Datos Abiertos presenta las siguientes características:

% \begin{itemize}
%     \item Los elementos de información, en adelante recursos (concept), se identifican mediante una URI.
%     \item Cada recurso se encuentra asociado a una categoría (elemento code de concept).
%     \item Los recursos se encuentran agrupados en conjuntos de datos (elemento dataset) accesibles en formato JSON a través de un URI indicada en el atributo id del elemento dataset.
%     \item En un dataset puede haber información sobre recursos asociados a varias categorías.
%     \item Los recursos de una categoría pueden estar accesibles a través de diferentes datasets.
%     \item Para la categorización de los recursos se utiliza un sistema de clasificación jerárquico basando en características temáticas.
% \end{itemize}

% Esta información se encuentra descrita en un Catálogo de Datos en formato XML (\hyperref[sec:catalogoxml]{\textit{catalogo.xml}}) válido con respecto al esquema \hyperref[sec:catalogoxsd]{\textit{catalogo.xsd}}.
% La aplicación por desarrollar proporcionará una herramienta de búsqueda que posibilite la recuperación de recursos asociados a un código de categoría generando un documento XML con los resultados.
% La \autoref{fig:Practica3fig1.jpg} muestra un ejemplo de presentación de parte de la estructura jerárquica de los concepts del catálogo.

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=\textwidth]{imagenes/Practica3fig1.jpg}
%     \caption{Representación de la estructura jerárquica de los concepts del catálogo de datos.}
%     \label{fig:Practica3fig1.jpg}
% \end{figure}


% La aplicación por desarrollar recibirá como argumento el criterio de búsqueda, esto es, el código de la categoría de la que se desea información, y proporcionará información sobre los concepts y datasets pertinentes, aplicando para ello los siguientes criterios:

% \begin{itemize}
%     \item Se considerarán pertinentes el concept cuyo código (elemento code) coincida con el criterio de búsqueda y todos los concepts descendientes del mismo.
%     \item Se considerarán pertinentes los dataset que contengan información asociada a alguno de los concept pertinentes (contengan un elemento concept con el atributo id igual al atributo id del elemento concept pertinente).
% \end{itemize}

% La \autoref{fig:Practica3fig2.jpg} muestra un ejemplo de búsqueda del concept con código 0097-022 y los resultados que se obtendrían.

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=\textwidth]{imagenes/Practica3fig2.jpg}
%     \caption{Concepts y datasets pertinentes para el código 0097-022.}
%     \label{fig:Practica3fig2.jpg}
% \end{figure}

\noindent La tercera práctica de la asignatura PIAT %Esta práctica 
se centra en el desarrollo de una aplicación que extrae información específica de un fichero XML proporcionado por el Portal de Datos Abiertos del Ayuntamiento de Madrid (http://datos.madrid.es). Este portal publica información sobre organismos, eventos, actividades y otros recursos, utilizando un formato JSON para los conjuntos de datos y un catálogo en formato XML para describir la estructura de la información.

El reto principal reside en la naturaleza jerárquica y categorizada de la información. Los recursos se clasifican mediante un sistema jerárquico de códigos, y se agrupan en conjuntos de datos ---datasets--- accesibles a través de URIs. Un mismo dataset puede contener información sobre recursos de diferentes categorías, y los recursos de una misma categoría pueden estar distribuidos en varios datasets.

Para navegar por esta estructura, la práctica se basa en un Catálogo de Datos en formato XML ---contenido en el archivo catalogo.xml--- validado por el esquema catalogo.xsd. Este catálogo describe la estructura jerárquica de las categorías ---concepts--- y sus códigos asociados.

La \autoref{fig:Practica3fig1.jpg} muestra un ejemplo de presentación de parte de la estructura jerárquica de los concepts del catálogo.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{imagenes/Practica3fig1.jpg}
    \caption{Representación de la estructura jerárquica de los concepts del catálogo de datos.}
    \label{fig:Practica3fig1.jpg}
\end{figure}

La aplicación a desarrollar debe funcionar como una herramienta de búsqueda que, dado un código de categoría como criterio de búsqueda, sea capaz de:

\begin{itemize}
  \item Identificar los concepts pertinentes: Esto incluye el concept cuyo código coincide con el criterio de búsqueda y todos sus descendientes en la jerarquía del catálogo.
  \item Identificar los datasets pertinentes: Se consideran pertinentes aquellos datasets que contienen información asociada a cualquiera de los concepts identificados en el paso anterior.
  \item Generar un documento XML con los resultados: Este documento debe incluir información sobre los concepts y datasets pertinentes.
\end{itemize}

Por ejemplo, al buscar el código \lstinline|0097-022|, la aplicación debe identificar el concept con ese código, así como todos sus concepts descendientes. A continuación, debe identificar todos los datasets que contienen información sobre cualquiera de estos concepts. Finalmente, debe generar un documento XML que contenga la información de los concepts y datasets pertinentes.

La \autoref{fig:Practica3fig2.jpg} muestra un ejemplo de búsqueda del concept con código \lstinline|0097-022| y los resultados que se obtendrían.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{imagenes/Practica3fig2.jpg}
    \caption{Concepts y datasets pertinentes para el código 0097-022.}
    \label{fig:Practica3fig2.jpg}
\end{figure}

Esta práctica sienta las bases para las prácticas 4 y 5, donde se profundizará en el análisis y procesamiento de la información extraída del XML y del JSON perteneciente al conjunto de datos mencionado anteriormente. La correcta identificación de los concepts y datasets pertinentes en esta etapa es crucial para el éxito de las siguientes prácticas.

\subsection{Desarrollo de la práctica}

\noindent La implementación de la Práctica 3 se basa en la creación de un analizador de documentos XML denominado XMLParser, desarrollado en JavaCC. Este analizador se ha diseñado para procesar documentos XML que contienen datos publicados en el Portal de Datos Abiertos del Ayuntamiento de Madrid (http://datos.madrid.es).
Los documentos XML que se procesan a través de esta herramienta presentan ciertas características clave, como la identificación de recursos mediante URI, la asociación a categorías, la agrupación en conjuntos de datos y una clasificación jerárquica basada en características temáticas. La tarea principal del analizador es extraer información específica relacionada con códigos de categorías y, posteriormente, generar un nuevo documento XML que contiene los resultados deseados.

El primer paso para resolver la práctica es definir la estructura del analizador que seguirá el procesamiento de los documentos XML. Esta estructura determina cómo se identifican y extraen los datos de los documentos, así como las acciones correspondientes a realizar en cada caso. La \autoref{fig:estructuraanalizadorp3} ilustra esta estructura:

% \textbf{\#TODO: Terminar foto XML y XML}


% \textbf{Figura 1: Estructura del analizador XML. Procesamiento de conceptos y datasets.}

% \textbf{\#TODO: Crear estructura}
\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{imagenes/estructuraanalizadorp3.png}
	\caption{\label{fig:estructuraanalizadorp3}Estructura del analizador XML. Procesamiento de conceptos y datasets.}
\end{figure}

Esta figura es muy similar a la \autoref{fig:analizadorlexico}, que ilustraba el análisis léxico de JavaCC. En este caso, la figura refleja la estructura del análisis al completo para esta práctica, en la que se observa cómo se procesan los elementos concept y dataset. 
% Se observa cómo se manejan los distintos elementos y atributos dentro de cada uno, de tal forma que se describe la estructura de forma detallada.
Puede visualizar la extracción de información y validación de los campos resultados en la figura \autoref{fig:p3validacion}. De igual manera, en la sección \hyperref[sec:codigofuentep3]{\textit{3.7.7. Código Fuente}} se visualizan los ficheros \lstinline|catalogo.xml| y \lstinline|XMLParser.jj|

 En aquellos no-terminales en los que se quiera ejecutar alguna acción, como puede ser añadir los conceptos y datasets a las listas pertinentes, o crear nuevos objetos de tipo Concept o Dataset, se define una acción léxica dentro del no-terminal, como por ejemplo:

\begin{lstlisting}

Concept concept() : {}
{
  <OPEN_CONCEPT> <ID> idValue = <STRING> <END_ELEMENT>
  <OPEN_CODE> codeValue = getCode() <CLOSE_CODE>
  <OPEN_LABEL> labelValue = getValue() <CLOSE_LABEL>
  (
    <OPEN_CONCEPTS>
    (
      childConcept = concept()
      {
        concepts.add(childConcept);
      }
    )+
    <CLOSE_CONCEPTS>
  )?
  <CLOSE_CONCEPT>
  {
    if(codeValue.equals(nombreCategoria)) {
      conceptLabel = idValue.image;
      conceptsList.add(new Concept(idValue.image, codeValue, labelValue, concepts));
    }
    return new Concept(idValue.image, codeValue, labelValue, concepts);
  }
}

\end{lstlisting}

En este fragmento de código se observa el no-terminal \lstinline|Concept()|, el cual define la estructura XML que debe tener un \lstinline|<concept>|. Si nos fijamos detenidamente en el procesamiento del no-terminal, podemos apreciar que se realizan dos acciones léxicas: una para añadir a una lista de concepts un concepto hijo: \lstinline|concepts.add(childConcept);|, y otra que realiza la comparación del valor de \lstinline|<code>| con el nombre de la categoría para crear un objeto concept con los campos extraídos.

\phantom{text}

\noindent \textbf{Estructura del Analizador XMLParser}

\phantom{text}

\noindent El archivo XMLParser.jj define la estructura y funcionamiento del analizador XML mediante JavaCC. A continuación, se describe detalladamente cada componente y su función, proporcionando una comprensión completa del desarrollo y el diseño del analizador.

En las opciones del analizador se definen configuraciones importantes como la concurrencia:

\lstset{inputencoding=utf8/latin1}
\begin{lstlisting}
options
{
  STATIC = false;
}
\end{lstlisting}

Estas opciones configuran el analizador para que no genere métodos estáticos, permitiendo la concurrencia y el procesamiento de múltiples entradas de manera eficiente.

Una vez definidas las opciones del analizador, comienza la declaración del parser y la definición de la clase \lstinline|XMLParser|:

\lstset{inputencoding=utf8/latin1}
\begin{lstlisting}
    
PARSER_BEGIN(XMLParser)
package piat.opendatasearch;
import java.util.List;
import java.util.ArrayList;
public class XMLParser {
    private List<Concept> conceptsList;
    private List<Dataset> datasetsList;
    private String conceptLabel;
    private String nombreCategoria;
}
PARSER_END(XMLParser)
    
\end{lstlisting}

en la que se observa que se instancia una lista de objetos \lstinline|Concept|, en la que se almacenarán todos los Concept cuyo código coincida con el criterio de la búsqueda, así como los \lstinline|Concept| recursivos que puedan tener.
Además se observa otra lista, en este caso de objetos \lstinline|Dataset|, en la que igualmente se almacenarán aquellos objetos Dataset que contengan algún un elemento concept con el atributo id igual al atributo \lstinline|id| del algún elemento concept de \lstinline|conceptsList|.
También se observa la creación de los atributos \lstinline|\lstinline|conceptLabel| y \lstinline|nombreCategoría|, encargados de almacenar el valor de \lstinline|<label>| en cada concept, y de almacenar el valor del nombre de la categoría pasado como argumento al programa principal, respectivamente.

Acto seguido, se definen los tokens que identifican las diferentes etiquetas y elementos del documento XML, así como las reglas para omitir espacios en blanco y caracteres irrelevantes:

\lstset{inputencoding=utf8/latin1}
\begin{lstlisting}
SKIP :
{
    " "
| "\t"
| "\r"
| "\n"
}

TOKEN :
{
    < OPEN_CONCEPTS : "<concepts>" >
| < OPEN_DATASETS : "<datasets>" >
| < CATALOG : "<catalog" >
| < XMLNS : "xmlns=" >
| < XMNLNS_XSI : "xmlns:xsi=" >
| < XSI_SCHEMA_LOCATION : "xsi:schemaLocation=" >
| < HEADER : "<?xml version=\"1.0\" encoding=\"UTF-8\"?>" >
| < OPEN_CONCEPT : "<concept" >
| < OPEN_DATASET : "<dataset" >
...
| < VALUE_CODE : ">" (~[ "\"", "\n", "\r", "<", ">", "=" ])+ "<" >
| < VALUE_LABEL : "<![CDATA[" (~[ "\"", "\n", "\r", "<", ">", "=" ])+ "]]>" >
| < STRING : "\"" (~[ "\"", "\n", "\r" ])+ "\"" >
| < END_ELEMENT : ">" >
}

SKIP :
{
    < ~[ ] >
}
    
\end{lstlisting}

Las reglas de omisión aseguran que los espacios en blanco, tabulaciones y saltos de línea no interfieran con el análisis, permitiendo que el analizador se concentre únicamente en los elementos significativos del XML. Se observa también que en caso de que el analizador detecte cualquier cosa que no sea un token que hayamos definido, se omite dicho elemento:

\lstset{inputencoding=utf8/latin1}
\begin{lstlisting}
SKIP :
{
    < ~[ ] >
}
\end{lstlisting}

De esta forma, cualquier token que no hayamos definido se omitirá. Como se estudio en \hyperref[sec:tokensenjavacc]{\textit{Capítulo 2. Estado del arte. JavaCC. Tokens en JavaCC}}, se define este token el último ya que este token acepta cualquier carácter, por lo que si no lo ponemos como el último token, se aplicaría la regla 2 y se omitiría todo carácter en el documento, lo que no nos permitiría recoger los token que hemos definido.

\tipbox{En este caso se observa que no se ha empleado ningún estado léxico, esto es debido a que se ha preferido realizar una comprobación estructural del documento ---asi como haría un xsd, aunque de forma mucho mas limitada---. En la practica 5 veremos un analizador XML en el que se sí se hace uso de estados léxicos, y veremos como esta funcionalidad acelera en gran medida el procesamiento.
}

Una vez visto el análisis léxico, toca centrarse en el analizador sintáctico. Este comienza con el método processFile, que recibe como argumento un código de categoría y procesa el archivo XML, extrayendo los conceptos y conjuntos de datos pertinentes:

\lstset{inputencoding=utf8/latin1}
\begin{lstlisting}

ManejadorXML processFile(String code) :
{
    conceptsList = new ArrayList<Concept>();
    datasetsList = new ArrayList<Dataset>();
    nombreCategoria = code;
}
{
    < HEADER > 
    < CATALOG > < XMLNS > < STRING > < XMNLNS_XSI > < STRING > < XSI_SCHEMA_LOCATION > < STRING > < END_ELEMENT >
    (< OPEN_CONCEPTS > (concept())+ < CLOSE_CONCEPTS >)+
    (< OPEN_DATASETS > (dataset())+ < CLOSE_DATASETS >)+
    < CLOSE_CATALOG >
    {
    return new ManejadorXML(datasetsList, conceptsList);
    }
}
    
\end{lstlisting}

Este método inicializa las listas de conceptos y conjuntos de datos, y establece el nombre de la categoría que se utilizará como criterio de búsqueda. Luego, procesa el documento XML siguiendo la estructura definida, identificando y extrayendo los conceptos y conjuntos de datos pertinentes. Al finalizar el procesamiento, retorna un objeto \lstinline|ManejadorXML| que contiene las listas de conceptos y conjuntos de datos extraídos.
\tipbox{
    Es importante destacar el sentido que tiene utilizar la clase ManejadorXML, si el parser que estamos desarrollando ya busca lo que quiere. El sentido que tiene es cronológico, ya que primero el parser realiza el análisis del documento y devuelve los datos, si se llama a una función que devuelva los datos es posible que se llamen a estas funciones antes de que se llame a la función de análisis.
}

Como se puede observar en el método, hace referencia a dos no-terminales, los métodos \lstinline|concept()| y \lstinline|dataset()|.
El método \lstinline|concept()| procesa cada concepto, extrayendo su código, label y posibles conceptos hijos. Utiliza recursividad para manejar estructuras jerárquicas de conceptos, asegurando que se capturen todas las relaciones parentales y de descendencia.

\lstset{inputencoding=utf8/latin1}
\begin{lstlisting}
    
Concept concept() :
{
  Token idValue;
  String codeValue, labelValue;
  Concept childConcept; 
  List < Concept > concepts = new ArrayList < Concept > ();
}
{
  < OPEN_CONCEPT > < ID > idValue = < STRING > < END_ELEMENT > 
  < OPEN_CODE > codeValue = getCode() < CLOSE_CODE > 
  < OPEN_LABEL > labelValue = getValue() < CLOSE_LABEL >
  (
    < OPEN_CONCEPTS >
    (
      childConcept = concept()
      {
        concepts.add(childConcept);
      }
    )+
    < CLOSE_CONCEPTS >
  )?
  < CLOSE_CONCEPT >
  {
    if(codeValue.equals(nombreCategoria)) {
      conceptLabel = idValue.image;
      conceptsList.add(new Concept(idValue.image, codeValue, labelValue, concepts));
	}
	return new Concept(idValue.image, codeValue, labelValue, concepts);
  }
}

\end{lstlisting}

Concretamente, el método concept se encarga de identificar cada elemento \lstinline|<concept>| en el documento XML. Extrae el valor del atributo \lstinline|id|, el código del concepto y el label del concepto. Si el código del concepto coincide con el criterio de búsqueda \lstinline|nombreCategoria|, se agrega a la lista de conceptos pertinentes \lstinline|conceptsList|. Además, si el concepto contiene conceptos hijos, el método se llama recursivamente para procesar estos hijos y construir una estructura jerárquica completa.

La recursividad se utiliza en el método \lstinline|concept()| para manejar la estructura jerárquica de los conceptos. Al encontrar un elemento \lstinline|<concepts>| dentro de un \lstinline|<concept>|, el método se llama a sí mismo para procesar cada \lstinline|<concept>| hijo. Este proceso se repite recursivamente hasta que se hayan procesado todos los conceptos descendientes, construyendo así una representación completa de la jerarquía de conceptos en el documento XML.

Por otra parte, si nos centramos en el método \lstinline|dataset()|, este  procesa cada conjunto de datos, extrayendo su título, descripción y conceptos asociados. Es responsable de identificar los conjuntos de datos que contienen información relevante para los conceptos identificados previamente.

\lstset{inputencoding=utf8/latin1}
\begin{lstlisting}
    
void dataset() :
{
    Token idDataset, idConcept;
    String title;
    String description = "";
    String keyword = "";
    String theme = "";
    String publisher = "";
    List < IdConcept > idConcepts = new ArrayList < IdConcept > ();
} 
{
    < OPEN_DATASET > < ID > idDataset = < STRING > < END_ELEMENT > 
    < OPEN_TITLE > title = getValue() < CLOSE_TITLE > 
    (< OPEN_DESCRIPTION > description = getValue() < CLOSE_DESCRIPTION >)?
    (< OPEN_KEYWORD >  keyword = getValue() < CLOSE_KEYWORD >)?
    (< OPEN_THEME > theme = getValue() < CLOSE_THEME >)?
    (< OPEN_PUBLISHER > publisher = getValue() < CLOSE_PUBLISHER >)?
    (
    < OPEN_CONCEPTS >
    (
        < OPEN_CONCEPT > < ID > idConcept = < STRING > < END_ELEMENT >
        {
        idConcepts.add(new IdConcept(idConcept.image));
        }
    )+
    < CLOSE_CONCEPTS >
    )?
    < CLOSE_DATASET >
    {
    for(IdConcept ic : idConcepts) {
        if(ic.getId().equals(conceptLabel)) {
        datasetsList.add(new Dataset(idDataset.image, title, description, keyword,theme,publisher,idConcepts));
        }
    }
    }
}   

\end{lstlisting}

Como se puede observar, este no-terminal se encarga de procesar los elementos \lstinline|<dataset>| en el documento XML. Extrae el id, el título y otros atributos opcionales como \lstinline|description|, \lstinline|keyword|, theme y publisher. También identifica los conceptos asociados a cada conjunto de datos y los agrega a una lista de identificadores de conceptos \lstinline|idConcepts|. Si alguno de estos identificadores coincide con el conceptLabel pertinente, el conjunto de datos se agrega a la lista de conjuntos de datos pertinentes \lstinline|datasetsList|.

Además, el analizador cuenta con los métodos auxiliares \lstinline|getCode()| y \lstinline|getLabel()|. Estos métodos auxiliares extraen el valor de los códigos y labels de los elementos XML. Son utilizados por los métodos concept y dataset para obtener los valores necesarios de los elementos XML.

\lstset{inputencoding=utf8/latin1}
\begin{lstlisting}

String getCode() :
{
    Token t = new Token();
}
{
    (t = < VALUE_CODE >)?
    {
    return t.image.substring(1, t.image.length() - 1);
    }
}

String getValue() :
{
    Token t = new Token();
}
{
    (t = < VALUE_LABEL >)?
    {
    return t.image;
    }
}  

\end{lstlisting}

El método \lstinline|getCode()| extrae el valor del código de un elemento \lstinline|<code>| eliminando los caracteres \lstinline|<| y \lstinline|>|, mientras que el método \lstinline|getValue()| extrae el valor de un elemento \lstinline|<label>| utilizando la sintaxis CDATA.

\phantom{text}

\noindent \textbf{Funcionalidad del Analizador}

\phantom{text}

% Como se ha visto, XMLParser está diseñado para cumplir con los siguientes requisitos específicos de la práctica:

% Extracción de conceptos relevantes: Identificar y extraer los conceptos cuyo código coincide con el criterio de búsqueda proporcionado, así como todos los conceptos descendientes de estos.
% Identificación de conjuntos de datos pertinentes: Determinar los conjuntos de datos que contienen información relacionada con los conceptos relevantes y extraer esta información.
% Generación de resultados en XML: Crear un nuevo documento XML que incluya los conceptos y conjuntos de datos extraídos, estructurados de manera que faciliten la búsqueda y el análisis posterior.

% El uso de JavaCC para desarrollar este analizador permite una mayor precisión y eficiencia en el procesamiento de documentos XML complejos. JavaCC, al ser una herramienta de generación de compiladores, permite definir de manera precisa las reglas de análisis sintáctico y léxico necesarias para interpretar documentos XML de forma efectiva. Esto es particularmente útil para manejar la complejidad y la jerarquía de los datos XML, asegurando que todos los elementos y relaciones se procesen correctamente.

% Además, la implementación en Java facilita la integración del analizador con otros sistemas y aplicaciones, permitiendo reutilizar el código y extender la funcionalidad según sea necesario. La capacidad de Java para manejar grandes volúmenes de datos y su amplia biblioteca estándar proporcionan una base robusta para el desarrollo de herramientas de análisis de datos.

% En resumen, la implementación de esta práctica proporciona una herramienta poderosa para analizar y extraer información de documentos XML, cumpliendo con los objetivos establecidos y demostrando la efectividad del uso de JavaCC en el desarrollo de analizadores de documentos XML. Esta herramienta no solo cumple con los requisitos de la práctica, sino que también ofrece un ejemplo claro de cómo utilizar técnicas avanzadas de análisis sintáctico y léxico para resolver problemas complejos de procesamiento de datos.

Como se ha visto, XMLParser se ha diseñado para cumplir con los requisitos específicos de la práctica, que incluyen la extracción de conceptos relevantes y la identificación de conjuntos de datos pertinentes.

Una de las funciones principales del analizador es extraer los conceptos que coinciden con el criterio de búsqueda proporcionado, así como todos sus conceptos descendientes. Además, determina los conjuntos de datos que contienen información relacionada con estos conceptos y los extrae para su posterior procesamiento.

Para facilitar la manipulación y el análisis de los datos extraídos, el analizador genera resultados en XML. Esto se realiza creando un nuevo documento XML estructurado de manera que facilite la búsqueda y el análisis posterior de los conceptos y conjuntos de datos.

La implementación del analizador se realiza utilizando JavaCC, lo que permite una mayor precisión y eficiencia en el procesamiento de documentos XML complejos. JavaCC es una herramienta de generación de compiladores que permite definir con precisión las reglas de análisis sintáctico y léxico necesarias para interpretar los documentos XML de manera efectiva. Esta capacidad es crucial para manejar la complejidad y la jerarquía de los datos XML, asegurando un procesamiento correcto de todos los elementos y relaciones.

Además, la implementación en Java facilita la integración del analizador con otros sistemas y aplicaciones. Esto permite reutilizar el código y extender la funcionalidad según sea necesario. La capacidad de Java para manejar grandes volúmenes de datos y su amplia biblioteca estándar proporcionan una base sólida para el desarrollo de herramientas de análisis de datos.

En conclusión, la implementación de esta práctica proporciona una herramienta versátil para analizar y extraer información de documentos XML. No solo cumple con los objetivos establecidos, sino que también demuestra la efectividad del uso de JavaCC en el desarrollo de analizadores de documentos XML. Esta herramienta ofrece un ejemplo claro de cómo utilizar técnicas avanzadas de análisis sintáctico y léxico para resolver problemas complejos de procesamiento de datos.

% \subsubsection{Preguntas Frecuentes}


% \subsubsection{Notas}

% \tipbox{
% hacer clase concept con atributos code y label, hacer lista concepts de concept. hacer lo mismo con dataset. recoger los datos al igual que hacíamos en NXCalculator
% Concepts, son objetos distintos, se tienen que reconocer de manera diferente. -> Nuevo objeto conceptInDataset
% No hace falta un estado léxico, simplemente crear un objeto nuevo ya que son cosas diferentes, -> IncludedConcepts
% String id -> idConcepts
% XMLParser devuelve un objeto que tenga implementado la interfaz ParserCatalogo
% }

% \noindent Como se ha comentado en la sección anterior, la principal herramienta desarrollada en esta práctica es el analizador XMLParser. Este analizador es capaz de llevar a cabo varias tareas esenciales:

% \begin{itemize}
%     \item Validación de argumentos de entrada para garantizar que los parámetros cumplan con las especificaciones requeridas.
%     \item Extracción de información relevante de los documentos XML, siguiendo las reglas y estructuras definidas en la gramática.
%     \item Generación de documentos de resultados que cumplen con un esquema específico.
% \end{itemize}
% El analizador XMLParser representa una contribución significativa en términos de procesamiento y manipulación de documentos XML.

\subsection{Validación de resultados}

En cuanto a tiempo de ejecución no se aprecian diferencias significativas, el principal aspecto a destacar en esta práctica es la simplicidad con la que se genera el analizador y el potencial de recursividad que este tiene. 
Suponga que se encuentra desarrollando esta práctica utilizando la herramienta SAX. Si la prácticas se modificase ---ligeramente--- y el catalogo.xml tuviese una recursividad mas profunda, esto complicaría mucho el procesamiento Java para algo que a priori no cambia mucho el formato del fichero que se quiere analizar. En otras palabras, un pequeño cambio en el documento a analizar implicaría un gran cambio del programa principal o clase encargada de analizar dicho fichero utilizando SAX.

Sin embargo, empleando JavaCC este problema no existiría. Esto es debido a la recursividad inherente que proporciona la plataforma, permitiendo que dentro de un no-terminal se encuentre especificado ese mismo no-terminal ---lo hemos visto en este capítulo con el método \lstinline|concept()|, y lo vimos también en el ejemplo de Calculadora explicado en \hyperref[sec:cap2]{\textit{Capítulo 2. Estado del arte}}, con los métodos \lstinline|Expression()| y \lstinline|Factor()|---. Esto significa que el grado de complejidad del archivo a analizar es inversamente proporcional al grado de complejidad de la gramática a desarrollar en JavaCC.

Por otra parte, es fundamental realizar una comparación visual del XML generado para verificar que la información extraída del catálogo se haya almacenado correctamente, con la estructura y los valores esperados. La \autoref{fig:p3validacion} muestra el resultado de la transformación, prestando especial atención a la correspondencia de los elementos del XML, así como a la integridad de los datos.

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{imagenes/p3validacion.png}
	\caption{\label{fig:p3validacion}Estructura del analizador XML. Procesamiento de conceptos y datasets.}
\end{figure}

% \textbf{\#TODO: mencionar ejemplos concretos de las ventajas de JavaCC sobre SAX en términos de recursividad y manejo de cambios en la estructura del XML...}

Un ejemplo concreto de las ventajas de JavaCC sobre SAX en términos de recursividad se puede observar en la gestión de los elementos anidados dentro del archivo catalogo.xml. Con JavaCC, la recursividad se maneja de forma natural mediante la definición de una regla gramatical que se llama a sí misma para procesar los elementos hijos. Si la estructura del archivo XML cambiase para incluir un nuevo nivel de anidamiento, la gramática JavaCC se podría adaptar fácilmente añadiendo una nueva regla recursiva. Con SAX, sin embargo, este cambio requeriría una modificación sustancial del código, con la adición de nuevos bucles y estructuras de control para manejar el nuevo nivel de anidamiento

% \textbf{\#TODO: Añadir gráfica comparativa con el cambio que se tendría que hacer en javacc y en sax}

En cuanto al manejo de cambios en la estructura del XML, JavaCC ofrece una mayor flexibilidad que SAX. Si se añadiera un nuevo atributo a los elementos , por ejemplo, la gramática JavaCC se podría modificar fácilmente para incluir este nuevo atributo en la definición del token correspondiente. Con SAX, este cambio requeriría la modificación del código que procesa los eventos de inicio y fin de elemento, lo que podría ser más complejo y propenso a errores.

Además, JavaCC permite definir de manera sintáctica el documento objetivo, funcionalidad que SAX no implementa.

\subsection{Conclusiones}

\noindent La Práctica 3 ha brindado a los estudiantes una experiencia valiosa en el manejo de tecnologías de análisis de documentos XML. A través del uso de JavaCC y la tecnología SAX, se ha proporcionado una base sólida para procesar información estructurada de manera eficiente. Los estudiantes han tenido la oportunidad de aplicar conceptos relacionados con la validación de argumentos, el análisis de elementos XML y la generación de documentos de resultados.
Uno de los aspectos más destacados de esta práctica es la capacidad de filtrar y seleccionar información relevante basada en códigos de categorías. Esta habilidad es fundamental en situaciones donde la extracción selectiva de datos es esencial.


\subsection{Código Fuente}
\label{sec:codigofuentep3}

\noindent El código fuente de los archivos elaborados en esta practica se encuentra a continuación para su estudio y aprendizaje. Cabe destacar que se proporcionan dicho código para que el lector sea capaz de probar las funcionalidades con el objetivo de aprender mas acerca de las aplicaciones de JavaCC.

No se promueve el plagio y la realización de las prácticas, este simplemente es una de las muchas formas en las que se podría realizar la práctica.

\hyperref[sec:XMLParser]{\textbf{XMLParser.jj}}

\hyperref[sec:catalogoxml]{\textbf{Catalogo.xml (Simplificado)}}
%\href{https://shorturl.at/alqN6}{XMLParser.jj}

\section{Análisis de archivos JSON. Práctica 4}

% \noindent La Práctica 4 se centra en la exploración y aplicación de la tecnología GSON Streaming para el análisis y procesamiento eficiente de documentos JSON. El objetivo principal de esta etapa es familiarizar a los estudiantes con la API GSON Streaming y desarrollar un analizador de documentos JSON basado en esta tecnología. Como objetivo secundario, se busca capacitar a los estudiantes en el diseño de algoritmos eficientes para la extracción y transformación de información proveniente de fuentes de contenidos estructurados.

\noindent La Práctica 4 tradicionalmente se ha centrado en la tecnología GSON Streaming para analizar y procesar documentos JSON de manera eficiente. El objetivo principal era familiarizar a los estudiantes con la API GSON Streaming y desarrollar un analizador JSON basado en ella. Como objetivo secundario, se buscaba capacitar a los estudiantes en el diseño de algoritmos eficientes para la extracción y transformación de información proveniente de fuentes de contenidos estructurados.

Sin embargo, en este proyecto proponemos un enfoque diferente: sustituir GSON Streaming por JavaCC. Esta decisión se fundamenta en las ventajas que ofrece JavaCC en términos de control, eficiencia y flexibilidad a la hora de analizar datos JSON.

\subsection{GSON Streaming}

% \noindent La API de transmisión de GSON es una API de Java que permite leer y escribir JSON de forma secuencial. Esto la hace útil en situaciones donde no es posible o deseable cargar el modelo de objeto completo en memoria, como cuando se trabaja con grandes cantidades de datos o cuando los datos se reciben de forma continua.

% La API de transmisión de GSON se basa en dos clases principales: JsonReader y JsonWriter. JsonReader se utiliza para leer JSON de forma secuencial, mientras que JsonWriter se utiliza para escribir JSON de forma secuencial.

% Las características principales de la API son su eficiencia en términos de memoria y  su flexibilidad. Por una parte, la API de transmisión de GSON no necesita cargar el modelo de objeto completo en memoria, lo que la hace más eficiente en términos de memoria que la API de GSON tradicional. Además, GSON Streaming permite leer y escribir datos JSON de forma secuencial, lo que la hace muy flexible.

% La API es ideal para trabajar con grandes cantidades de datos, ya que puede leer y escribir los datos sin necesidad de cargarlos todos en memoria. También es ideal para recibir datos de forma continua, ya que puede leer los datos a medida que se reciben.

\noindent La API de transmisión de GSON permite leer y escribir JSON de forma secuencial en Java. Resulta útil cuando no es factible o deseable cargar el modelo de objeto completo en memoria, por ejemplo, al trabajar con grandes volúmenes de datos o cuando los datos se reciben de forma continua.

Las clases principales de la API son JsonReader y JsonWriter, que se utilizan para leer y escribir JSON secuencialmente, respectivamente. GSON Streaming destaca por su eficiencia en términos de memoria y su flexibilidad. Al no requerir la carga completa del modelo de objeto en memoria, resulta más eficiente que la API tradicional de GSON. Además, su capacidad de lectura y escritura secuencial la hace muy flexible.


\begin{figure}[H]
  \centering
  \begin{lstlisting}
    // Instanciar un objeto JsonReader
    JsonReader jsonReader = new JsonReader(new StringReader(json));
    
    try {
    // Mientras haya un elemento...
    while (jsonReader.hasNext()) {
      // ...se mira el siguiente token -sin consumirlo-...
      JsonToken nextToken = jsonReader.peek();
      // ...y se hacen las comprobaciones
      if (JsonToken.BEGIN_OBJECT.equals(nextToken)) {
        jsonReader.beginObject();
      } else if (JsonToken.NAME.equals(nextToken)) {
        String name = jsonReader.nextName();
        System.out.println("Token KEY >>>> " + name);
      } else if ...
    \end{lstlisting}
  \caption{\label{fig:jsonreaderejemplo}Ejemplo de análisis utilizando la clase JsonReader de GSON Streaming API\cite{ejemplojsonreader}}
  \end{figure}


Como se puede observar en la \autoref{fig:jsonreaderejemplo}, el análisis con GSON Streaming implica codificar una lógica muy general y con un alto nivel de bucles y condiciones. Además, este análisis realiza un procesamiento de todos los datos del archivo.

\subsection{JavaCC vs. GSON Streaming}

% Mientras que GSON Streaming se basa en un enfoque de lectura secuencial, lo que puede resultar eficiente para grandes volúmenes de datos, JavaCC ofrece la posibilidad de crear analizadores sintácticos personalizados con un control granular sobre el proceso de análisis.

% Esta característica resulta fundamental en este contexto, ya que permite la extracción selectiva de la información relevante de los archivos JSON, optimizando el uso de recursos y mejorando el rendimiento general del sistema.

% Extracción selectiva: A diferencia de GSON Streaming, que procesa todo el flujo de datos, JavaCC permite definir qué elementos y atributos son relevantes, optimizando el proceso de análisis.
% Rendimiento: Al evitar el procesamiento innecesario de información, se reduce el consumo de recursos y se mejora el rendimiento, especialmente al tratar con grandes conjuntos de datos.
% Flexibilidad: JavaCC ofrece un alto grado de flexibilidad para adaptarse a diferentes estructuras de archivos JSON, lo que lo convierte en una herramienta versátil para el análisis de datos.

 \noindent A diferencia del enfoque de lectura secuencial de GSON Streaming, JavaCC permite crear analizadores sintácticos personalizados con un control granular sobre el proceso de análisis. Esta característica es crucial en este contexto, ya que permite la extracción selectiva de información relevante de los archivos JSON, optimizando el uso de recursos y mejorando el rendimiento general del sistema.

Las ventajas de JavaCC frente a GSON Streaming en este escenario son:

\begin{itemize}
  \item Extracción selectiva: JavaCC permite definir qué elementos y atributos son relevantes, optimizando el proceso de análisis y evitando el procesamiento innecesario de información, a diferencia de GSON Streaming, que procesa todo el flujo de datos.
  
  \item Rendimiento: Al evitar el procesamiento innecesario, se reduce el consumo de recursos y se mejora el rendimiento, especialmente al tratar con grandes conjuntos de datos.
  
  \item Flexibilidad: JavaCC ofrece un alto grado de flexibilidad para adaptarse a diferentes estructuras de archivos JSON, lo que lo convierte en una herramienta versátil para el análisis de datos.
\end{itemize}


\subsection{Descripción de la práctica}

\noindent La práctica busca ampliar la funcionalidad de la herramienta de búsqueda del Portal de Datos Abiertos del Ayuntamiento de Madrid, desarrollada en la Práctica 3. Se pretende dotar a esta herramienta de la capacidad para extraer información sobre los recursos asociados a una categoría específica ---concept--- del portal. Esto se logrará accediendo a los conjuntos de datos ---dataset--- en formato JSON y procesándolos con un analizador GSON Streaming.

% El código desarrollado en la Práctica 3 servirá como punto de partida, y se completará para integrar un analizador GSON Streaming. Este analizador procesará la información de cada conjunto de datos pertinente y generará un documento XML válido conforme al esquema de documento ResultadosBusquedaP4.xsd. El nuevo elemento introducido en este esquema es resources, que contendrá información sobre los recursos asociados a las categorías pertinentes.

% La generación del documento XML (indicado por ARG2) será similar al proceso de la Práctica 3, pero ahora se incorporará el elemento resources. Se utilizará la clase JSONDatasetParser para implementar el analizador GSON Streaming, y se analizarán los archivos .json indicados en el atributo id de cada dataset. Se añadirán como máximo cinco recursos (resource) a partir de cada dataset analizado.


Para ello, se utilizará JavaCC para crear un analizador sintáctico que, a diferencia de GSON Streaming, permitirá la extracción selectiva de los datos necesarios, optimizando el proceso. La información extraída se utilizará para generar un documento XML válido conforme al esquema ResultadosBusquedaP4.xsd, que incluye un nuevo elemento "resources" para almacenar la información sobre los recursos.

% \textbf{\#TODO: Comprobar cual de los dos textos me gusta mas }

% En esta práctica, ampliaremos la funcionalidad de la herramienta de búsqueda del Portal de Datos Abiertos del Ayuntamiento de Madrid, desarrollada en la Práctica 3. El objetivo es dotarla de la capacidad de extraer información sobre los recursos asociados a una categoría específica (concept) del portal.

% Para ello, accederemos a los conjuntos de datos (dataset) en formato JSON y los procesaremos con un analizador JavaCC. La información extraída se utilizará para generar un documento XML válido conforme al esquema ResultadosBusquedaP4.xsd, que incluye un nuevo elemento "resources" para almacenar la información sobre los recursos.



\subsection{Desarrollo de la práctica}

% La implementación con JavaCC se basa en la definición de una gramática que refleje la estructura de los archivos JSON a analizar. Esta gramática permitirá al analizador identificar los elementos relevantes y extraer la información deseada.

% \subsubsection{Preguntas Frecuentes}

% \noindent Pregunta 1: ¿Cuál es el propósito principal de la Práctica 4 en términos de procesamiento de información JSON?
% Respuesta: El objetivo principal es extender la herramienta de búsqueda para extraer información sobre recursos asociados a categorías específicas en el Portal de Datos Abiertos, utilizando la tecnología GSON Streaming.

% Pregunta 2: ¿Qué elemento se introduce en el esquema Resultados\\BusquedaP4.xsd ?
% Respuesta: Se introduce el elemento resources, que contendrá información sobre los recursos asociados a las categorías pertinentes.

% Pregunta 3: ¿Cuál es la versión de la API GSON Streaming utilizada en esta práctica?
% Respuesta: Se utiliza la versión 2.9.0 de la API GSON Streaming, disponible en este enlace.


% \noindent La herramienta principal desarrollada en esta práctica es la extensión de la herramienta de búsqueda del Portal de Datos Abiertos, ahora mejorada con la capacidad de analizar y procesar datos JSON mediante la API GSON Streaming. La clase JSONDatasetParser representa la implementación de este analizador GSON Streaming. La herramienta final permite la extracción selectiva de información de archivos JSON, generando un documento XML conforme al esquema ResultadosBusquedaP4.xsd que incluye el nuevo elemento resources.

\noindent La implementación con JavaCC se basa en la definición de una gramática que refleje la estructura de los archivos JSON a analizar. Esta gramática, definida en el archivo JSONParser.jj, permitirá al analizador identificar los elementos relevantes dentro de los datasets JSON del portal de datos abiertos y extraer la información deseada para la generación del XML.

El primer paso consiste en definir los tokens que nos interesan. Dado que solo buscamos información específica dentro de cada recurso del dataset, ignoraremos la mayoría de los elementos del JSON. Para ello, se definen estados léxicos en la gramática que se activarán al encontrar los tokens que marcan el inicio y fin de las secciones relevantes. La \autoref{fig:estadoslexicosp4} resume los diversos estados léxicos utilizados.

\begin{lstlisting}
TOKEN :
{
  < GRAPHS : "\"@graph\"" > : GRAPH
}
< GRAPH > 
TOKEN :
{
  < IDRESOURCE : "\"@id\"" > : GETVALUE
| < ARRAY_START : "[" >
| < ARRAY_END : "]" > : DEFAULT
} 

< RESOURCE >
TOKEN :
{
  < IDCONCEPT : "\"@type\"" > : GETVALUE
| < TITLE : "\"title\"" > : GETVALUE
| < DESCRIPTION : "\"description\"" > : GETVALUE
| < TIME_START : "\"dtstart\"" > : GETVALUE
| < TIME_END : "\"dtend\"" > : GETVALUE
| < LINK : "\"link\"" > : GETVALUE
| < EVENT_LOCATION : "\"event-location\"" > : GETVALUE
| < AREA : "\"area\"" > : INAREA
| < LATITUDE : "\"latitude\"" > : GETVALUE
| < LONGITUDE : "\"longitude\"" > : GETVALUE
| < ORGANIZATION_NAME : "\"organization-name\"" > : GETVALUE
| < ACCESIBILITY : "\"accesibility\"" > : GETVALUE
| < ADDRESS : "\"address\"">
| < LOCATION : "\"location\"">
| < ORGANIZATION : "\"organization\"">
}

< RESOURCE>
SKIP:
{
  < "},\n	{"  > : GRAPH
| < "}\n						\n	}">: GRAPH
}

< GETVALUE >
TOKEN :
{
  < STRING : "\"" (~[ "\"", "\n", "\r" ])* "\"" > : RESOURCE
| < INTEGER : ("-")? ([ "0"-"9" ])+ > : RESOURCE
| < DOUBLE : ("-")? ([ "0"-"9" ])+("." ([ "0"-"9" ])+) >: RESOURCE
| < TRUE : "true" > : RESOURCE
| < FALSE : "false" > : RESOURCE
| < NULL : "null" > : RESOURCE
}

< INAREA >
TOKEN :
{
  < IDAREA : "\"@id\"" > : GETVALUE
}

< * >
SKIP :
{
  < ~[ ] >
}
\end{lstlisting}

% \textbf{\#TODO: AÑADIR TABLA ESTADOS LÉXICOS}

El análisis comienza en el estado por defecto buscando el token \lstinline|@graph|. Una vez encontrado, se activa el estado \lstinline|GRAPH| que busca el inicio de un recurso mediante el token \lstinline|@id|. Al encontrarlo, se activa el estado \lstinline|RESOURCE| y se comienza la extracción de la información.

\lstset{inputencoding=utf8/latin1}
\lstinputlisting{code/processfile.txt}

Dentro del estado \lstinline{RESOURCE}, se definen los tokens que representan los campos que queremos obtener: \lstinline{title}, \lstinline{description}, \lstinline{dtstart}, \lstinline{dtend}, \lstinline{link}, \lstinline{event-location}, \lstinline{area}, \lstinline{latitude}, \lstinline{longitude}, \lstinline{organization-name} y \lstinline{accesibility}.

\begin{lstlisting}
  Resource resource() :
  {
    Resource r = new Resource();
    Token t;
  }
  {
    (< IDRESOURCE > t = < STRING >){r.setResourceId(t.image);}
    (< IDCONCEPT > t = < STRING >)?{r.setConceptId(t.image);}  
    (< TITLE >  t = < STRING >){r.setTitle(t.image);}
    (< DESCRIPTION > t = < STRING >){r.setDescription(t.image);}
    (< TIME_START > t = < STRING > {r.setStartDate(t.image); })
    (< TIME_END > t = < STRING > {r.setEndDate(t.image); })
    (< LINK >  t = < STRING >){r.setLink(t.image);}
    (< EVENT_LOCATION > t = < STRING >){r.setEvntLoc(t.image);}
    (< ADDRESS > 
      (< AREA>(< IDAREA > t = < STRING >){r.setAr(t.image);})?)?
    (< LOCATION >
      (< LATITUDE >  t = < DOUBLE >)
      {if(!t.image)r.setLat(Double.parseDouble(t.image));}
      (< LONGITUDE >  t = < DOUBLE >)
      {if(!t.image)r.setLong(Double.parseDouble(t.image));})?
    (< ORGANIZATION >
    (< ORGANIZATION_NAME > t = < STRING >){r.setOrganizationName(t.image);}
    (< ACCESIBILITY >  t = < STRING >){r.setAcces(t.image);})? 
    { return r; }
  }
\end{lstlisting}

Para manejar el valor de estos campos, se utiliza el estado léxico \lstinline|GETVALUE|. Este estado se activa al encontrar cualquiera de los tokens mencionados anteriormente y se encarga de capturar el valor asociado, ya sea un \lstinline|STRING|, \lstinline|INTEGER|, \lstinline|DOUBLE|, \lstinline|TRUE| , \lstinline|FALSE| o \lstinline|NULL|.

\begin{figure}[H]
  \centering
  \begin{tabularx}{\textwidth}{>{\bfseries}l X X}
  \toprule
  \textbf{Estado Léxico} & \textbf{Descripción} & \textbf{Función} \\
  \midrule
  DEFAULT & Estado inicial & Reconoce tokens comunes como espacios en blanco y saltos de línea \\
  % \midrule
  % IN\_HEADER & Activado al reconocer el primer elemento de la cabecera & Reconoce los diferentes elementos que componen el encabezado XML \\
  \midrule
  GRAPH & Activado al encontrar \lstinline|@graph| & Busca el inicio de un recurso mediante \lstinline|@id| \\
  \midrule
  RESOURCE & Activado al encontrar \lstinline|@id| & Extrae la información del recurso \\
  \midrule
  GETVALUE & Activado al encontrar un token que requiere obtener su valor & Captura el valor asociado al token \\
  \midrule
  INAREA & Activado al encontrar \lstinline|area| & Busca el token \lstinline|@id| dentro de \lstinline|area| \\
  \bottomrule
  \end{tabularx}
  % \caption{Descripción de los Estados Léxicos}
  \caption{\label{fig:estadoslexicosp4}Práctica 4. Estados léxicos del analizador}
  \label{table:lexical_statesp4}
  \end{figure}

Un aspecto importante es la gestión del campo \lstinline|area|. Este campo contiene a su vez un identificador que nos interesa capturar. Para ello, al encontrar el token \lstinline|area||, se activa el estado léxico \lstinline|INAREA| que busca específicamente el token \lstinline|@id| para obtener el valor del identificador del área.

La gramática utiliza una serie de reglas de producción para definir cómo se combinan los tokens para formar estructuras válidas. La regla principal, \lstinline|processFile()|, se encarga de iterar sobre la lista de recursos del JSON, extrayendo la información de cada uno y almacenándola en una lista de objetos Resource.

Para optimizar el proceso, se ha establecido un límite máximo de objetos a procesar. Una vez alcanzado este límite, la función processFile finaliza, deteniendo el análisis del JSON.

% La figura 9 muestra un ejemplo de la información extraída de un archivo JSON y su posterior transformación a XML. Se puede observar cómo la gramática de JavaCC permite seleccionar y estructurar la información de manera precisa, incluyendo únicamente los campos relevantes para nuestro objetivo.

% Para evaluar el rendimiento de JavaCC en comparación con GSON Streaming, se han realizado pruebas de análisis con archivos JSON de diferentes tamaños. La figura 10 muestra los tiempos de ejecución obtenidos.

% \textbf{\#TODO: INSERTAR IMAGEN DE TIEMPOS DE EJECUCIÓN Y AÑADIR EXPLICACIÓN}

% Como se puede observar en la figura 10, JavaCC demuestra un mejor rendimiento que GSON Streaming, especialmente al procesar archivos JSON de gran tamaño. Esto se debe a que JavaCC realiza un análisis selectivo del archivo, extrayendo únicamente la información necesaria, mientras que GSON Streaming procesa el flujo completo de datos.

\subsection{Validación de resultados}

La validación de los resultados se realizará contrastando la información extraída con la información original de los archivos JSON. 
% Se prestará especial atención a la precisión y completitud de los datos extraídos, así como al rendimiento del analizador implementado con JavaCC.

La \autoref{fig:p4xmlvalidacion2} muestra un ejemplo de la información extraída de un archivo JSON y su posterior transformación a XML. Se puede observar cómo la gramática de JavaCC permite seleccionar y estructurar la información de manera precisa, incluyendo únicamente los campos relevantes para nuestro objetivo.

% \textbf{\#TODO: Terminar foto JSON y XML}


% \begin{figure}[H]
% 	\centering
% 	\includegraphics[width=\textwidth]{imagenes/p4jsonvalidacion2.png}
% 	\caption{\label{fig:p4jsonvalidacion2}Práctica 4. Fichero JSON que analiza JavaCC}
% \end{figure}

% \begin{figure}[H]
% 	\centering
% 	\includegraphics[width=\textwidth]{imagenes/p4xmlvalidacion.png}
% 	\caption{\label{fig:p4xmlvalidacion2}Práctica 4. Fichero XML que genera el programa}
% \end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{imagenes/p4validacion.png}
	\caption{\label{fig:p4xmlvalidacion2}Práctica 4. Fichero JSON que analiza JavaCC y fichero XML que genera el programa}
\end{figure}


% \textbf{[COMPLETAR] INSERTAR IMAGEN JSON Y XML}

% Se espera que la utilización de JavaCC en lugar de GSON Streaming demuestre las ventajas de esta herramienta en términos de eficiencia, flexibilidad y control sobre el proceso de análisis de datos JSON.

Para evaluar el rendimiento de JavaCC en comparación con GSON Streaming, se han realizado pruebas de análisis con archivos JSON %de diferentes tamaños
. La \autoref{fig:comparacion_tiempos_ejecucion_p4} muestra los tiempos de ejecución obtenidos.

% \textbf{\#TODO: INSERTAR IMAGEN FICHEROS FUENTE PARA VER LO SIMPLE QUE ES, DECIR QUE PODRÍA HABER MUCHA MAS DIFERENCIA }

% \textbf{[COMPLETAR]TIEMPOS...}
% \textbf{[COMPLETAR]INSERTAR IMAGEN TIEMPOS...EXPLICAR}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{imagenes/comparacion_tiempos_ejecucion_p4.png}
	\caption{\label{fig:comparacion_tiempos_ejecucion_p4}Comparación de Tiempos de Ejecución: GSON vs JavaCC}
\end{figure}

% La figura 10 muestra una comparativa de tiempos de ejecución entre GSON Streaming y JavaCC para esta tarea.
%  Como 
% %  se puede observar
% hemos comentado anteriormente
%  , JavaCC ofrece un rendimiento significativamente superior, especialmente al procesar archivos JSON de gran tamaño. Esto se debe a que JavaCC realiza un análisis selectivo del archivo, extrayendo únicamente la información necesaria, mientras que GSON Streaming procesa el flujo completo de datos. 
 
%  \textbf{\#TODO: Añadir porcentajes, explicar porque es asi la gráfica.}

 Como 
%  se puede observar
hemos comentado anteriormente
, JavaCC ofrece un rendimiento significativamente superior, especialmente al procesar archivos JSON de gran tamaño.
 En la gráfica de la \autoref{fig:comparacion_tiempos_ejecucion_p4}, se observa que JavaCC es aproximadamente un 57\% más rápido que GSON Streaming al procesar archivos JSON de gran tamaño. Esta diferencia se debe a que JavaCC realiza un análisis selectivo del archivo, extrayendo únicamente la información necesaria, mientras que GSON Streaming procesa todo el flujo de datos. La gráfica compara la ejecución de esta práctica utilizando la herramienta correspondiente, en la que se observa %muestra la tendencia lineal en el tiempo de ejecución de ambas herramientas, pero la pendiente de 
 que la línea de JavaCC es significativamente menor que la de GSON Streaming, lo que indica una mayor eficiencia en el procesamiento de grandes volúmenes de datos.

%  \textbf{\#TODO: arreglar sección}


\subsection{Conclusiones}

% \noindent La Práctica 4 ha proporcionado a los estudiantes una oportunidad única para aplicar sus conocimientos adquiridos en la Práctica 3 y explorar en profundidad la API GSON Streaming. La capacidad de extender la funcionalidad de la herramienta existente para manejar datos JSON y generar documentos XML enriquecidos demuestra una comprensión avanzada de las tecnologías de procesamiento de datos estructurados.

% El uso de GSON Streaming versión 2.9.0 ha permitido a los estudiantes implementar un analizador eficiente para manejar grandes conjuntos de datos JSON, cumpliendo así con los requisitos de la práctica.

La elección de JavaCC como herramienta principal para el análisis de datos en esta práctica no es casual. En la siguiente práctica ---así como se hizo en la anterior--- se abordará la extracción de información de documentos XML utilizando XPath. La versatilidad de JavaCC permitirá, en la práctica 5, sustituir también esta herramienta, creando un flujo de trabajo unificado y simplificado para el procesamiento de información en diferentes formatos.

% La elección de JavaCC como herramienta principal para el análisis de datos en esta práctica no es casual. En la siguiente práctica, al igual que en la anterior, se abordará la extracción de información de documentos XML utilizando XPath. La versatilidad de JavaCC permitirá, en la práctica 5, sustituir también esta herramienta, creando un flujo de trabajo unificado y simplificado para el procesamiento de información en diferentes formatos.


\subsection{Código Fuente}

\noindent El código fuente de los archivos elaborados en esta practica se encuentra a continuación para su estudio. Cabe destacar que se proporcionan dicho código para que el lector sea capaz de probar las funcionalidades con el objetivo de aprender mas acerca de las aplicaciones de JavaCC.

No se promueve el plagio y la realización de las prácticas, este simplemente es una de las muchas formas en las que se podría realizar la práctica.

\hyperref[sec:JSONParser]{\textbf{JSONParser.jj}}
%\href{https://shorturl.at/wFG45}{JSONParser.jj}

\section{Análisis de archivos XML (2). Práctica 5}

% La Práctica 5 representa una etapa crucial en el proyecto, enfocándose en la familiarización con la tecnología de transformación de documentos XML mediante el Lenguaje de Rutas de XML (XPath), basado en el Modelo de Objetos de Documento (DOM). El objetivo principal es extraer información del documento XML generado en la Práctica 4 utilizando expresiones XPath y almacenarla en un nuevo documento en formato JSON. Esta práctica permite a los estudiantes explorar técnicas avanzadas de manipulación de datos estructurados.

\noindent La Práctica 5 marca un punto de inflexión en este proyecto, al introducir la transformación de documentos XML mediante XPath (XML Path Language) y el Modelo de Objetos de Documento (DOM). El objetivo principal es extraer información específica del documento XML generado en la Práctica 4 y representarla en formato JSON. Este proceso permite explorar técnicas avanzadas de manipulación de datos estructurados y sienta las bases para la integración con otras herramientas y sistemas.

Sin embargo, en lugar de depender de XPath para la extracción de datos, en este proyecto se propone la utilización de JavaCC como herramienta única para el análisis y procesamiento del documento XML. Esta elección se justifica por la eficiencia, flexibilidad y control que ofrece JavaCC, permitiéndonos prescindir de herramientas adicionales como XPath, SAX o GSON.

\subsection{XPath}

XPath, abreviatura de XML Path Language, es un lenguaje de consulta para documentos XML. Se utiliza para seleccionar partes de un documento XML, como elementos, atributos, texto y datos binarios.

XPath se basa en una sintaxis similar a la de las expresiones regulares. Las expresiones XPath se utilizan para construir caminos a través de un documento XML.

Para ilustrar su uso, consideremos el siguiente fragmento de XML:

\lstset{inputencoding=utf8/latin1}
\begin{lstlisting}
<catalog>
    <concepts>
        <concept
            id="https://datos.madrid.es/egob/kos/...">
            <code>0001-018</code>
            <label><![CDATA[CiudaDistrito]]></label>
        </concept>
    </concepts>
</catalog>

\end{lstlisting}

Si nuestro objetivo fuera seleccionar el código del primer concept, la expresión XPath que utilizaríamos sería:

\lstset{inputencoding=utf8/latin1}
\begin{lstlisting}
/catalog/concepts/concept/code
\end{lstlisting}

Esta expresión realiza un recorrido por el documento desde la raíz \lstinline|/| hasta el elemento \lstinline|catalog|, para luego buscar el elemento \lstinline|concepts|, el elemento \lstinline|concept| y finalmente seleccionar su hijo \lstinline|code|.

Las principales características de XPath son las siguientes:

\begin{itemize}
  \item Selección de elementos: XPath se puede utilizar para seleccionar elementos individuales o conjuntos de elementos en un documento XML.
  \item Selección de atributos: XPath se puede utilizar para seleccionar atributos individuales o conjuntos de atributos de un elemento.
  \item Selección de texto: XPath se puede utilizar para seleccionar texto de un elemento o atributo.
  \item Selección de datos binarios: XPath se puede utilizar para seleccionar datos binarios de un elemento o atributo.
\end{itemize}

XPath se utiliza en una variedad de aplicaciones, incluyendo el procesamiento de XML, el desarrollo web, y la integración de datos. Esto permite que se utilize para procesar documentos XML, extraer datos, validar documentos y transformar documentos. Además, se utiliza en aplicaciones web para acceder a datos XML, como datos de formularios o datos de un servicio web. Por último, también se puede emplear para integrar datos de diferentes fuentes, como datos XML y datos de bases de datos.

\subsection{JavaCC vs. XPath}

\noindent Tanto JavaCC como XPath pueden emplearse para procesar documentos XML, sin embargo, cada uno presenta características que los hacen más adecuados para diferentes escenarios.

JavaCC es una herramienta de generación de analizadores sintácticos. Se utiliza para generar analizadores sintácticos que pueden analizar documentos XML.

Como ya hemos visto en secciones anteriores, las principales ventajas de JavaCC son su eficiencia en términos de memoria y su gran flexibilidad a los usuarios. Por otra parte, la principal desventaja de JavaCC es su dificultad de uso,

XPath es un lenguaje de consulta para documentos XML. Se utiliza para seleccionar partes de un documento XML.

Las principales ventajas de XPath son su sencillez, ya que es una herramienta relativamente sencilla de aprender a utilizar, y su generalidad, pues se puede utilizar para procesar cualquier documento XML, independientemente de su lenguaje.

Las principales desventajas de XPath son su eficiencia, debido a que es menos eficiente en términos de memoria que JavaCC, y sus limitaciones, como la imposibilidad de procesar datos binarios.

% \textbf{[COMPLETAR]...en xpath se procesa muchas veces el fichero...xpath: procesa muchas veces el fichero. contar el numero de recursos a cada identificador, varias expresiones xpath , que te va a obligar a ejecutar el fichero cada vez...
% }

En XPath, para obtener información específica, se deben ejecutar múltiples expresiones de consulta sobre el documento XML. Cada expresión XPath recorre el árbol del documento, lo que implica un acceso repetitivo al fichero. Por ejemplo, para contar el número de recursos asociados a cada identificador, además del contenido del título de cada recurso, se necesitarían ejecutar dos expresiones XPath. Este proceso puede resultar ineficiente en términos de rendimiento, especialmente al tratar con documentos XML grandes, o si se quiere extraer bastante información. JavaCC, por otro lado, analiza el documento una sola vez, extrayendo la información necesaria de forma estructurada y eficiente.

\subsection{Descripción de la práctica}

\noindent Esta práctica se basa en la extensión del código desarrollado en la Práctica 4, sustituyendo la utilización de XPath por un nuevo analizador desarrollado en JavaCC. El objetivo principal es procesar un nuevo fichero XML, extrayendo información específica que se almacenará en un documento JSON.

Para lograr este objetivo, se implementará un nuevo analizador léxico y sintáctico en JavaCC que nos permita:

\begin{itemize}
  \item Identificar los elementos relevantes: El analizador debe ser capaz de reconocer las etiquetas y atributos específicos del documento XML que contienen la información a extraer.
  \item Extraer la información: Una vez identificados los elementos relevantes, el analizador debe extraer su contenido y almacenarlo en estructuras de datos apropiadas.
  \item Generar el documento JSON: Finalmente, el analizador debe utilizar la información extraída para generar un nuevo documento en formato JSON con la estructura deseada.
\end{itemize}


% \noindent La implementación de la Práctica 5 se basa en extender el código desarrollado en la Práctica 4, integrando la funcionalidad de XPath para extraer información específica del documento XML resultante. El uso del Modelo de Objetos de Documento (DOM) facilita la navegación y manipulación de la estructura jerárquica del documento XML.

% Se requerirá la introducción de expresiones XPath para identificar y seleccionar los elementos deseados en el documento XML. La información obtenida mediante XPath se almacenará en un nuevo documento en formato JSON. Este proceso de transformación garantiza que la información relevante se extraiga eficientemente y se represente en un formato JSON para su posterior análisis o intercambio.

% En esta práctica, se llevará a cabo la evaluación de las sentencias XPath sobre el documento XML de salida generado en la práctica anterior. Este documento será utilizado para realizar varias consultas específicas que permitirán extraer datos relevantes. Las consultas XPath incluirán la obtención del contenido textual del elemento <query>, el conteo de elementos <dataset> hijos de <datasets>, la extracción del contenido de cada elemento <title> hijo de <resource>, y la identificación del número de elementos <resource> cuyo atributo id coincide con el atributo id del elemento <dataset>.

% A la hora de implementar esta funcionalidad utilizando XPATH, en el guión de la práctica se recomienda desarrollar una nueva clase  responsable de evaluar las expresiones XPath y devolver una colección con los resultados. La estos resultados serán convertido a formato JSON. Finalmente, el documento JSON generado contendrá todos los resultados de las evaluaciones XPath en el formato especificado.

% Además, en el guión se menciona que el proyecto continuará utilizando las clases implementadas en prácticas anteriores ---en nuestro caso, implementaremos los mismos parsers XML y JSON que vimos en las secciones de las prácticas 3 y 4 respectivamente---, y se integrará una nueva clase principal renombrada como P5\_XPATH, que incluirá el código necesario para cumplir con las nuevas especificaciones. La aplicación deberá recibir argumentos adicionales que incluyan la ruta al documento JSON de salida.

% Este ejercicio refuerza la familiarización con XPath y DOM, destacando la importancia de las tecnologías de transformación y manipulación de documentos XML en aplicaciones telemáticas. Además, la práctica provee una comprensión práctica de cómo estas tecnologías pueden ser utilizadas para realizar consultas avanzadas y transformar datos XML en formatos más manejables y adecuados para el análisis o intercambio de información.

\subsection{Desarrollo de la práctica}

\noindent La herramienta principal desarrollada en esta práctica es una extensión del código existente en la Práctica 4, sustituyendo la capacidad de utilizar expresiones XPath para extraer información específica del documento XML con un nuevo parser desarrollado en JavaCC para analizar un nuevo fichero XML. La información extraída se guarda en un nuevo documento en formato JSON, ofreciendo una representación alternativa y flexible de los datos.

A la hora de resolver la práctica, se pueden emplear diferentes enfoques. Una opción sería desarrollar un analizador que simplemente reconozca la estructura básica del documento XML, extrayendo la información de forma general sin tener en cuenta la semántica de los datos. Otra opción, más sofisticada, sería crear un analizador que comprenda la estructura y el significado de los datos, permitiendo una extracción más precisa y selectiva de la información relevante, permitiendo obviar aquellos elementos que no interesen. En este proyecto, se optará por el segundo enfoque, aprovechando la capacidad de JavaCC para definir gramáticas que reflejen la estructura y la semántica del documento XML.

En esta sección, vamos a describir cómo se resuelve esta práctica utilizando JavaCC y XPath, apoyándonos en el archivo XMLParser2.jj proporcionado.

\phantom{text}

\noindent \textbf{Configuración del Analizador}

\phantom{text}

Se mantienen las opciones del analizador utilizadas en las prácticas anteriores, garantizando la compatibilidad y la posibilidad de analizar las tres gramáticas --prácticas 3,4 y 5--- con el mismo parser.

\lstset{inputencoding=utf8/latin1}
\begin{lstlisting}
options
{
    STATIC = false; // para permitir entradas de manera dinámica
    JAVA_TEMPLATE_TYPE = "modern"; // uso de plantilla moderna de Java
}
\end{lstlisting}


Además, también hay que declarar del parser y la definir la clase XMLParser2:
\lstset{inputencoding=utf8/latin1}
\begin{lstlisting}
PARSER_BEGIN(XMLParser2)
package piat.opendatasearch;
import java.util.List;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.concurrent.atomic.AtomicInteger;
public class XMLParser2{
  private HashMap <String, AtomicInteger> datasetsMap;
  private List < String > titleList;
}
PARSER_END(XMLParser2) 
\end{lstlisting}

Esta clase contiene un mapa para almacenar los datasets ---\lstinline|datasetsMap|---%, una variable para contar el número de datasets (numDatasets), y 
,así como
una lista para almacenar los títulos ---\lstinline|titleList|---. La estructura de esta clase es fundamental para organizar y gestionar los datos extraídos de los documentos XML de manera eficiente.

La parte mas relevante de este ejercicio con JavaCC es la definición de los tokens y los estados léxicos. Estos permiten acotar el reconocimiento de tokens a secciones específicas del documento, optimizando el proceso de análisis.

\lstset{inputencoding=utf8/latin1}
\begin{lstlisting}
< * >
SKIP :
{
  " " 
| "\t"
| "\r"
| "\n"
} 

TOKEN :
{
  < HEADER : "<?xml version=\"1.0\" encoding=\"UTF-8\"?>" > 
| < XMLNS : "xmlns=" > 
| < XMNLNS_XSI : "xmlns:xsi=" > 
| < XSI_SCHEMA_LOCATION : "xsi:schemaLocation=" > 
| < SEARCH_RESULTS : "<searchResults" >
| < OPEN_QUERY : "<query>" > : LEX_CONTENT
| < CLOSE_DATASETS : "</datasets>" > 
| < OPEN_DATASET : "<dataset id=" > : IN_DATASET
| < OPEN_DATASETS : "<datasets>" > 
| < OPEN_RESOURCES : "<resources>" > : IN_RESOURCES
| < CLOSE_SEARCH_RESULTS : "</searchResults>" >
}

< IN_RESOURCES >
TOKEN:
{
  < CLOSE_RESOURCES : "</resources>" > : DEFAULT
| < OPEN_RESOURCE : "<resource id=" > 
| < CLOSE_RESOURCE : "</resource>" > 
| < OPEN_TITLE : "<title>" >  : LEX_CONTENT
}

< IN_DATASET, IN_RESOURCES >
TOKEN:
{
  < STRING : "\"" (~[ "\"", "\n", "\r" ])+ "\">">
| < CLOSE_DATASET : "</dataset>" > : DEFAULT
}

< LEX_CONTENT >
TOKEN:
{
  < CLOSE_QUERY : "</query>" > : DEFAULT
| < CLOSE_TITLE : "</title>" > : IN_RESOURCES
| < VALUE : (~["<"])+ > 
}

< * >
SKIP :
{
  < ~[ ] >
}
\end{lstlisting}

Como en las prácticas anteriores, omitimos los espacios en blanco, tabulaciones y saltos de linea. En este caso, incorporamos estados léxicos, que nos permiten reconocer un grupo de tokens en un punto específico del análisis. 
El uso principal de los estados léxicos en la práctica es para reconocer unicamente lo que nos interesa del documento, que es la cabecera del documento, el valor del elemento \lstinline|<query>|, el valor \lstinline|id| de cada dataset, y el valor \lstinline|title| de cada resource. 
% Los estados léxicos definidos, además del estado DEFAULT son:

% %% modificar!
% \begin{itemize}
%     \item IN\_HEADER: estado utilizado para reconocer únicamente elementos de la cabecera del archivo, como el espacio de nombres, la version XML, o la localización del XMLSchema. Se crea este estado principalmente para poder reconocer los strings dentro de la cabecera como se hacía en la práctica 3.
%     \item IN\_QUERY: estado utilizado para reconocer el contenido del elemento query.
%     \item IN\_TITLE: estado utilizado para reconocer el contenido del elemento title.
%     \item IN\_DATASETS: estado utilizado para reconocer los elementos id dentro de la estructura de un dataset
%     \item IN\_RESOURCES: Estado utilizado para reconocer el id de los recursos y el valor del elemento title dentro de la estructura de un resource
% \end{itemize}

El estado inicial es \lstinline|DEFAULT|, donde se reconocen tokens comunes como espacios en blanco y saltos de línea. Al encontrar un token que indica el inicio de una sección específica, el analizador cambia a un estado léxico específico para esa sección.

Por ejemplo, al encontrar el token \lstinline|<OPEN_QUERY>|, que marca el comienzo del del elemento \lstinline|<query>|, el analizador cambia al estado \lstinline|LEX_CONTENT|. Dentro de cada estado, se reconocen tokens específicos relacionados con esa sección.

El estado \lstinline|LEX_CONTENT| se utiliza para capturar el valor de la consulta, mientras que \lstinline|IN_DATASETS| y \lstinline|IN_RESOURCES| se utilizan para extraer información sobre los datasets y recursos, respectivamente.

% El estado IN\_TITLE, utilizado dentro de IN\_RESOURCES, permite obtener el título de cada recurso.

La gramática utiliza el token \lstinline|STRING| para capturar valores entre comillas y \lstinline|VALUE| para obtener texto dentro de etiquetas.

Finalmente, las funciones \lstinline|dataset()| y \lstinline|resource()| definen la lógica para procesar la información de cada dataset y recurso, almacenándola en las estructuras de datos definidas en la clase XMLParser2.

%% modificar
\begin{figure}[H]
  \centering
  \begin{tabularx}{\textwidth}{>{\bfseries}l X X}
  \toprule
  \textbf{Estado Léxico} & \textbf{Descripción} & \textbf{Función} \\
  \midrule
  DEFAULT & Estado inicial & Reconoce tokens comunes como espacios en blanco, saltos de línea, y el inicio de etiquetas \\
  % \midrule
  % IN\_HEADER & Activado al reconocer el primer elemento de la cabecera & Reconoce los diferentes elementos que componen el encabezado XML \\
  \midrule
  LEX\_CONTENT & Activado tras reconocer el comienzo de una etiqueta para obtener el contenido de ella & Reconoce el contenido de la etiqueta \\
  \midrule
  IN\_DATASETS & Activado después de reconocer \lstinline|datasets| & Reconoce tokens específicos de los datasets \\
  \midrule
  IN\_RESOURCES & Activado después de reconocer \lstinline|resources| & Reconoce tokens específicos de los recursos \\
  % \midrule
  % IN\_TITLE & Activado al encontrar \textless title\textgreater & Reconoce el contenido de la etiqueta \textless title\textgreater \\
  \bottomrule
  \end{tabularx}
  % \caption{Descripción de los Estados Léxicos}
  \caption{\label{fig:estadoslexicosp5}Práctica 5. Estados léxicos del analizador}
  \label{table:lexical_statesp5}
  \end{figure}

% \tipbox{
%   % \textbf{\#TODO: Completar sección}
%    Cabe destacar que se definen los estados léxicos mencionados para omitir el resto de caracteres o tokens que el analizador pueda reconocer en un momento dado. Por ejemplo, después de la cabera solo nos interesa el elemento query, por lo que definimos un estado léxico IN\_QUERY en el que definimos los tokens que nos permiten reconocer el valor de la query. De esta forma, se definen ciertos tokens que unicamente pueden o deben ser analizados en un punto concreto del análisis.
% }

Una vez se definen los tokens, resta generar el análisis sintáctico. El método principal \lstinline|processfile()| inicializa las estructuras de datos y procesa el archivo XML, extrayendo la información relevante y almacenándola en las estructuras adecuadas:

\lstset{inputencoding=utf8/latin1}
\begin{lstlisting}
ManejadorJSON processFile() : { String query; datasetsMap = new HashMap<String, AtomicInteger>(); titleList = new ArrayList<String>();}
{
  < HEADER > 
  < SEARCH_RESULTS > < XMLNS > < XMNLNS_XSI >  < XSI_SCHEMA_LOCATION > 
  < OPEN_QUERY > query = getContent() < CLOSE_QUERY > 
  < OPEN_DATASETS > (dataset())*  < CLOSE_DATASETS >
  < OPEN_RESOURCES > (resource())* < CLOSE_RESOURCES >
  < CLOSE_SEARCH_RESULTS >
  < EOF >
  {
    return new ManejadorJSON(query,datasetsMap.size(),datasetsMap,titleList);
  }
}

\end{lstlisting}

Este procesa el documento XML siguiendo la estructura definida, identificando y extrayendo los datos pertinentes. Al finalizar el procesamiento, imprime el número de datasets, el mapa de datasets, y la lista de títulos extraídos.

Además, se cuenta con el método \lstinline|dataset()| que procesa cada dataset, extrayendo su id y actualizando el mapa de datasets:

% \textbf{\#TODO: atomic integer en vez de integer. (integer es inmutable)...hablar de la concurrencia...}

\lstset{inputencoding=utf8/latin1}
\begin{lstlisting}
void dataset() : { String idDataset; } 
{
  < OPEN_DATASET >  idDataset = getAttribute() 
  < CLOSE_DATASET >
  {
    datasetsMap.put(idDataset, new AtomicInteger(0));
  }
}
\end{lstlisting}

% El método dataset identifica cada elemento \lstinline|<dataset>| en el documento XML, extrae su \lstinline|id| y lo agrega al mapa de datasets
% .
% , incrementando el contador de datasets.

Adicionalmente, también se cuenta con el método \lstinline|resource()|, que procesa cada recurso, extrayendo su id, el id del concepto asociado, y el título. Además, actualiza el contador de conceptos en el mapa de datasets:

\lstset{inputencoding=utf8/latin1}
\begin{lstlisting}
void resource() : { String title, idResource; } 
{
  < OPEN_RESOURCE >  idResource = getAttribute() 
  < OPEN_TITLE > title = getContent() < CLOSE_TITLE > 
  < CLOSE_RESOURCE >
  {
    titleList.add(title);
    final AtomicInteger cont = datasetsMap.get(idResource);
    if(cont != null)
      cont.incrementAndGet();
  }
}
\end{lstlisting}

, así como métodos adicionales para obtener los valores:

\lstset{inputencoding=utf8/latin1}
\begin{lstlisting}
String getContent() : { Token t = new Token(); }
{
  (t = < VALUE >)?
  {
    return t.image;
  } 
}

String getAttribute() : { Token t = new Token(); }
{
  t = < STRING > 
  {
    return t.image.replace("\"", "");
  } 
}
\end{lstlisting}


% \subsubsection{Preguntas Frecuentes}

% \noindent Pregunta 1: ¿Cuál es el propósito principal de la Práctica 5 en términos de transformación de datos?
% Respuesta: El objetivo principal es la extracción de información específica del documento XML mediante expresiones XPath y la posterior representación de esta información en un nuevo documento en formato JSON.

% Pregunta 2: ¿Cómo se realiza la transformación de datos de XML a JSON en esta práctica?
% Respuesta: La transformación se realiza mediante la utilización de expresiones XPath para identificar y seleccionar información específica en el documento XML generado en la Práctica 4. La información seleccionada se almacena en un nuevo documento en formato JSON.

% Pregunta 3: ¿Cómo se integra la funcionalidad XPath en el código existente de la Práctica 4?
% Respuesta: Se realizarán modificaciones en el código de la Práctica 4 para incorporar expresiones XPath que seleccionen la información deseada. La información extraída se utilizará para generar un nuevo documento en formato JSON.

\subsection{Validación de resultados}

Para validar los resultados de la Práctica 5, se llevan a cabo diversas comprobaciones. Se realiza una comparación visual del JSON generado para verificar que la información extraída del XML se haya almacenado correctamente, con la estructura y los valores esperados. Se presta especial atención a la correspondencia entre los elementos del XML y las claves del JSON, así como a la integridad de los datos.

Esta validación se observa en la \autoref{fig:p5validacion}
% \textbf{INSERTAR IMAGEN XML Y JSON}

% \textbf{\#TODO: Terminar foto XML y JSON}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{imagenes/p5validacion-2.png}
	\caption{\label{fig:p5validacion}Práctica 5. Fichero XML que analiza JavaCC y fichero JSON que genera el programa}
\end{figure}

En la \autoref{fig:p5validacionjsonrfc} se valida el formato JSON, utilizando un validador online para asegurar que el JSON generado cumple con la sintaxis correcta y no contiene errores.

% \textbf{[COMPLETAR] INSERTAR IMAGEN VALIDADOR ONLINE. añadir texto explicativo}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{imagenes/p5validacionjsonrfc.png}
	\caption{\label{fig:p5validacionjsonrfc}Práctica 5. Comprobación del archivo JSON generado en la herramienta online \textit{JSON Formatter \& Validator} }
\end{figure}

El validador online confirma que el JSON generado cumple con la sintaxis correcta, verificando la estructura de los objetos, la correspondencia entre llaves y valores, y el uso adecuado de los tipos de datos. Esta validación garantiza la integridad del JSON generado y facilita su posterior procesamiento por otras herramientas o sistemas.

Para garantizar la robustez de la solución, se prueba la aplicación con diferentes archivos XML de entrada, con diferentes estructuras y contenidos, asegurando así que la extracción de información mediante XPath funciona correctamente en diferentes escenarios.

Finalmente, se analiza el tiempo de ejecución de la aplicación con archivos XML de diferentes tamaños para comprobar que la extracción de información mediante XPath se realiza de forma eficiente. En aspecto no se observa una diferencia notable entre las dos herramientas.

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{imagenes/p5tiemposejecucion.png}
	\caption{\label{fig:p5tiemposejecucion}Comparación de Tiempos de Ejecución: XPath vs JavaCC}
\end{figure}

% \textbf{[COMPLETAR]INSERTAR GRÁFICO TIEMPOS JAVACC Y XPATH. COMPLETAR CON TEXTO
% }

Como se puede observar en la \autoref{fig:p5tiemposejecucion}, el tiempo de ejecución de ambas herramientas, JavaCC y XPath, aumenta de forma lineal con el tamaño del archivo XML. Si bien JavaCC presenta un tiempo ligeramente inferior para archivos pequeños, esta diferencia se reduce a medida que el tamaño del archivo aumenta.

En este caso particular, la ventaja de JavaCC en términos de rendimiento no es tan significativa como en la práctica 2 (análisis de logs). Esto se debe a que la estructura del archivo XML es relativamente simple y XPath puede realizar la extracción de información de forma eficiente.

% \textbf{\#TODO: hacer gráfica en P5 en la que se vea que cuantas mas expresiones XPATH se hagan, más tarda en ejecutarse (ya que analiza varias veces el archivo. Hacer una segunda gráfica en la que se diga antes que si aumenta el numero de información a analizar, ahí es donde xpath penca.
% Sin embargo, si se realizase un pequeño evolutivo de la práctica en la que se pidiese extraer mas información, se notaria en gran medida la diferencia entre herramientas:}

Como se observa en la \autoref{fig:p5tiemposexpresiones}, si se realizara un evolutivo de la práctica en el que se pidiese extraer más información, la diferencia entre ambas herramientas se haría más evidente, ya que JavaCC solo analiza el documento una vez, mientras que XPath tendría que realizar múltiples recorridos para obtener toda la información necesaria:

% \textbf{\#TODO: insertar gráfica}
\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{imagenes/xpathyjavaccexpresiones.png}
	\caption{\label{fig:p5tiemposexpresiones}Comparación de Tiempos de Ejecución: JavaCC vs XPath en Información a analizar}
\end{figure}


Sin embargo, es importante destacar que la simplicidad de la gramática JavaCC y su capacidad para adaptarse a cambios en la estructura del archivo XML la convierten en una alternativa atractiva a largo plazo. Si la estructura del archivo XML se volviera más compleja o se requiriera una mayor flexibilidad en el proceso de análisis, JavaCC podría ofrecer un mejor rendimiento y una mayor facilidad de mantenimiento que XPath.

\subsection{Conclusiones}

\noindent La Práctica 5 demuestra el potencial de JavaCC como herramienta única para el análisis y procesamiento de documentos XML en el contexto de las aplicaciones telemáticas. La capacidad de integrar el análisis léxico, el análisis sintáctico, la extracción de información y la generación de documentos en un único framework simplifica el desarrollo y ofrece un mayor control sobre el proceso. 

% \noindent La Práctica 5 proporciona a los estudiantes una valiosa experiencia en la utilización de JavaCC para la extracción selectiva de información de documentos XML. La capacidad de integrar esta tecnología con el código existente de la Práctica 4 demuestra la versatilidad en el manejo de datos estructurados.

% El proceso de transformación a JSON destaca la flexibilidad de la tecnología utilizada, permitiendo la representación de datos en diferentes formatos según las necesidades del proyecto.

La sustitución de XPath por JavaCC en esta práctica no solo simplifica la arquitectura de la solución, sino que también abre la puerta a la implementación de funcionalidades más complejas y personalizadas en el futuro.

\section{Evolutivo de las prácticas}

\subsection{Introducción}

% \noindent Como se ha podido observar de las prácticas de las secciones anteriores, pese a ser interesantes y cumplir con los objetivos establecidos de aprendizaje, son poco interesantes y se podría realizar unos ejercicios mas potentes y versátiles que exprimiesen al máximo las funcionalidades de las herramientas que hemos ido repasando.

% Es por ello que en esta sección se plantea unos ejercicios a modo de evolutivo, que amplían los requerimientos y complejidad del análisis. Además se observará que, gracias al incremento en la complejidad de los ejercicios, aumenta en proporción la sencillez con la que se resuelven dichos problemas empleando JavaCC.

% De esta forma, se podrá observar lo ventajoso de crear un analizador a la medida de usuario, y como JavaCC es una herramienta versátil y muy potente.

% \tipbox{
%     plantear un problema mas interesante y hacer un archivo mas complejo
% -las practicas son aburridas, proponer practicas mas interesantes, potentes, versátiles…para demostrar lo fácil que es con javacc, para que se vea la ventaja

% }

\noindent Las prácticas de PIAT, si bien cumplen con su objetivo de introducir a los estudiantes en el análisis y procesamiento de información en diferentes formatos, presentan un potencial de mejora en cuanto a la complejidad y el desafío que plantean. La simplicidad de las estructuras de datos utilizadas en las prácticas no permite apreciar completamente las ventajas de utilizar una herramienta como JavaCC, que destaca por su eficiencia y flexibilidad en el manejo de estructuras complejas y la adaptación a diferentes formatos.

En esta sección, se proponen evolutivos para las prácticas 3 y 5, con el objetivo de aumentar la complejidad del análisis y demostrar la superioridad de JavaCC frente a las herramientas tradicionales como SAX y XPath en escenarios más desafiantes. Estos evolutivos no solo buscan aumentar la dificultad de las prácticas, sino también proporcionar a los estudiantes una experiencia más enriquecedora y práctica que les permita comprender las ventajas de utilizar herramientas avanzadas para el procesamiento de información.

\subsection{Evolutivo 1. XML: Gestión de un Sistema de Bibliotecas}

% \noindent Este evolutivo se presenta a partir de la Practica 3. En dicha práctica se analizaban documentos xml, y se proponía analizar un documento xml con cierta recursividad

% Las prácticas proporcionadas en la asignatura, si bien son útiles para la introducción a las herramientas, pueden resultar algo básicas y poco desafiantes. Para demostrar la verdadera potencia y versatilidad de JavaCC, se propone la creación de un archivo XML con una estructura más compleja e intrincada, incluyendo múltiples niveles de anidamiento y una variedad de elementos y atributos. Este evolutivo permitirá a los estudiantes explorar las capacidades de JavaCC para manejar estructuras XML complejas de forma eficiente y elegante, resaltando las ventajas de utilizar un analizador personalizado frente a soluciones más genéricas.

Este evolutivo se basa en la Práctica 3, que se centra en el análisis de archivos XML. Se propone la creación de un sistema de gestión de bibliotecas utilizando un archivo XML con una estructura más compleja y recursiva que el catalogo.xml utilizado en la práctica original.

\subsubsection*{Estructura del archivo XML}

El archivo XML representará un sistema de bibliotecas interconectadas, con información sobre:

\begin{itemize}
  \item Bibliotecas: Nombre, dirección, datos de contacto, etc.
  \item Libros: Título, autor, ISBN, editorial, año de publicación, número de copias disponibles, etc.
  \item Usuarios: Nombre, DNI, dirección, teléfono, libros prestados, etc.
  \item Préstamos: Fecha de préstamo, fecha de devolución, usuario, libro, etc.
\end{itemize}

La estructura del archivo XML incluirá múltiples niveles de anidamiento y relaciones complejas entre los diferentes elementos. Por ejemplo, un libro puede estar asociado a varias bibliotecas, un usuario puede tener varios libros prestados y un préstamo puede estar asociado a un usuario y un libro específico.

\subsubsection*{Tareas a realizar}

Se propone la implementación de las siguientes funcionalidades utilizando JavaCC:

\begin{itemize}
  \item Búsqueda de libros: Permitir la búsqueda de libros por título, autor, ISBN o editorial, mostrando la información del libro y las bibliotecas donde está disponible.
  \item Gestión de usuarios: Permitir el alta, baja y modificación de usuarios, así como la consulta de información sobre un usuario específico, incluyendo los libros que tiene prestados.
  \item Gestión de préstamos: Permitir el registro de nuevos préstamos, la devolución de libros y la consulta de información sobre préstamos activos.
  \item Generación de informes: Generar informes estadísticos sobre el uso de las bibliotecas, como el número de libros prestados por biblioteca, el usuario con más préstamos activos, etc.
\end{itemize}

\subsubsection{Ventajas de JavaCC}

Manejo eficiente de la recursividad: JavaCC, gracias a su capacidad para definir gramáticas recursivas, puede analizar la estructura compleja del archivo XML de forma eficiente, sin necesidad de recurrir a bucles anidados o estructuras de control complejas como SAX.
Extracción selectiva de información: JavaCC permite definir qué elementos y atributos son relevantes para cada tarea, optimizando el proceso de análisis y evitando el procesamiento innecesario de información.
Flexibilidad y adaptabilidad: JavaCC permite modificar fácilmente la gramática para adaptarla a cambios en la estructura del archivo XML o a la incorporación de nuevas funcionalidades, lo que facilita el mantenimiento y la evolución del sistema.

\subsubsection{Comparación con SAX}

SAX, al basarse en un modelo de eventos, tendría dificultades para manejar la recursividad y las relaciones complejas del archivo XML. La implementación de las funcionalidades propuestas con SAX requeriría un código mucho más complejo y menos eficiente que con JavaCC, con una mayor probabilidad de errores y un mantenimiento más complicado.

\subsection{Evolutivo 2. XML y JSON: Análisis de Datos de Sensores}

Este evolutivo se basa en la Práctica 5, que se centra en la transformación de documentos XML a JSON. Se propone la creación de un sistema que analice datos de sensores almacenados en formato XML y los transforme a JSON para su posterior procesamiento.

\subsubsection*{Estructura del archivo XML}

El archivo XML contendrá información sobre:

\begin{itemize}
  \item Sensores: Tipo de sensor, ID, ubicación, etc.
  \item Mediciones: Fecha y hora de la medición, valor de la medición, unidad de medida, etc.
\end{itemize}
La estructura del archivo XML será jerárquica, con múltiples niveles de anidamiento para representar las relaciones entre los sensores y las mediciones.

\subsubsection*{Tareas a realizar}

Se propone la implementación de las siguientes funcionalidades utilizando JavaCC:

\begin{itemize}
  \item Extracción de datos: Extraer información específica del archivo XML, como el ID del sensor, el tipo de sensor, la fecha y hora de las mediciones y los valores de las mediciones.
  \item Transformación a JSON: Transformar la información extraída a un formato JSON estructurado, que incluya un array de objetos, cada uno representando un sensor con sus mediciones.
  \item Filtrado de datos: Permitir el filtrado de datos por tipo de sensor, rango de fechas o valores de las mediciones.
  \item Cálculo de estadísticas: Calcular estadísticas sobre los datos extraídos, como la media, la desviación estándar, el valor máximo y el valor mínimo de las mediciones para cada sensor.
\end{itemize}

\subsubsection{Ventajas de JavaCC}

Análisis eficiente y preciso: JavaCC, al generar analizadores sintácticos personalizados, puede analizar el archivo XML de forma eficiente y precisa, extrayendo la información relevante sin necesidad de recorrer todo el documento como XPath.
Control granular sobre el proceso de análisis: JavaCC permite definir con precisión qué elementos y atributos son relevantes para cada tarea, optimizando el proceso de análisis y evitando el procesamiento innecesario de información.
Integración con JSON: JavaCC facilita la transformación de los datos extraídos a JSON, permitiendo la creación de estructuras JSON complejas y anidadas.

\subsubsection{Comparación con XPath}

XPath, al basarse en expresiones de consulta, requeriría la definición de múltiples expresiones para extraer la información necesaria del archivo XML. Cada expresión XPath recorrería el árbol del documento, lo que implicaría un acceso repetitivo al fichero y un mayor consumo de recursos. Además, XPath no ofrece la misma flexibilidad que JavaCC para la transformación de datos a JSON, limitando la complejidad de las estructuras JSON que se pueden generar.


%%%

Los evolutivos propuestos para las prácticas 3 y 5 demuestran que JavaCC es una herramienta más potente y versátil que SAX y XPath para el análisis y procesamiento de información en escenarios complejos. La capacidad de JavaCC para manejar recursividad, realizar extracciones selectivas de información, adaptarse a diferentes formatos y generar código eficiente lo convierte en una opción ideal para el desarrollo de aplicaciones que requieren un alto grado de control y eficiencia en el manejo de datos.

Estos evolutivos no solo buscan aumentar la dificultad de las prácticas, sino también proporcionar a los estudiantes una experiencia más enriquecedora y práctica que les permita comprender las ventajas de utilizar herramientas avanzadas para el procesamiento de información. La incorporación de estos evolutivos al currículo de PIAT puede contribuir a la actualización de la asignatura y a la formación de profesionales mejor preparados para afrontar los desafíos del análisis y procesamiento de información en el mundo digital actual.

% \textbf{\#TODO: Añadir un evolutivo de la práctica 4.}

\subsection{Evolutivo para la Práctica 4: Análisis de Datos Geoespaciales en Tiempo Real}

Este evolutivo se basa en la Práctica 4, que se centra en el análisis de archivos JSON. Se propone la creación de un sistema que procese datos Geoespaciales en tiempo real provenientes de una fuente de streaming, como un servicio web o una base de datos. Los datos Geoespaciales se recibirán en formato JSON y contendrán información sobre la ubicación, el tiempo y otros atributos relevantes de diferentes eventos o entidades.

\subsubsection{Tareas a realizar}

Procesamiento de datos en tiempo real: Implementar un mecanismo para procesar el flujo continuo de datos JSON provenientes de la fuente de streaming.
Análisis geoespacial: Extraer información geoespacial relevante de los datos JSON, como la latitud, la longitud y otros atributos geoespaciales.
Visualización de datos: Representar los datos geoespaciales en un mapa interactivo, mostrando la ubicación de los eventos o entidades en tiempo real.
Alertas y notificaciones: Configurar alertas y notificaciones basadas en eventos geoespaciales específicos, como la entrada de una entidad en una zona geográfica determinada.
\subsubsection{Ventajas de JavaCC}

Eficiencia en el procesamiento de streaming: JavaCC, al generar analizadores eficientes, puede procesar el flujo continuo de datos JSON en tiempo real sin afectar el rendimiento del sistema.
Flexibilidad en la definición de gramáticas: JavaCC permite definir gramáticas que se adapten a la estructura específica de los datos JSON geoespaciales.
Integración con herramientas de visualización: JavaCC se puede integrar fácilmente con herramientas de visualización de mapas, como Leaflet o Google Maps, para representar los datos geoespaciales de forma interactiva.
Este evolutivo no solo aumenta la complejidad de la práctica, sino que también introduce a los estudiantes en el mundo del procesamiento de datos en tiempo real y el análisis geoespacial, habilidades cada vez más demandadas en el mercado laboral.